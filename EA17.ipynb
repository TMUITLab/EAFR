{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TMUITLab/EAFR/blob/master/EA17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjActARX7sCY",
        "outputId": "a4905bb9-b746-4eb7-ef73-08877428ef3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'GCN-Align'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Total 53 (delta 0), reused 0 (delta 0), pack-reused 53\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n",
            "Cloning into 'RREA'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 32 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "%cd '/content'\n",
        "!git clone https://github.com/1049451037/GCN-Align\n",
        "!git clone https://github.com/MaoXinn/RREA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZB2YBVkb4pYB"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from importlib.machinery import SourceFileLoader\n",
        "from google.colab import files\n",
        "\n",
        "layer = SourceFileLoader(\"layer\", \"/content/RREA/CIKM/layer.py\").load_module()\n",
        "utils = SourceFileLoader(\"utils\", \"/content/RREA/CIKM/utils.py\").load_module()\n",
        "CSLS = SourceFileLoader(\"CSLS\", \"/content/RREA/CIKM/CSLS.py\").load_module()\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import keras\n",
        "import pickle\n",
        "from scipy.sparse import hstack\n",
        "import math\n",
        "from tqdm import *\n",
        "import numpy as np\n",
        "from utils import *\n",
        "from CSLS import *\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.layers import *\n",
        "from layer import NR_GraphAttention\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYvCCmAe62rP",
        "outputId": "6f002c93-515f-44e7-f12a-3667e1778887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38960 3024\n"
          ]
        }
      ],
      "source": [
        "lang = 'zh'\n",
        "main_radj,train_pair,dev_pair,adj_matrix,r_index,r_val,adj_features,rel_features,nadj,char_size = load_data('/content/GCN-Align/data/%s_en/'%lang,train_ratio=0.30)\n",
        "train_pair_main=train_pair\n",
        "adj_matrix = np.stack(adj_matrix.nonzero(),axis = 1)\n",
        "main_rel_matrix,rel_val = np.stack(rel_features.nonzero(),axis = 1),rel_features.data\n",
        "ent_matrix,ent_val = np.stack(adj_features.nonzero(),axis = 1),adj_features.data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_size = rel_features.shape[1] \n",
        "\n",
        "\n",
        "# pair_dic = dict()\n",
        "# pairs = np.concatenate((dev_pair , train_pair),axis=0)\n",
        "# for i in range(len(pairs)):\n",
        "#     pair_dic[pairs[i][0]] = pairs[i][1]\n",
        "\n",
        "\n",
        "main_rel_matrix1 = main_rel_matrix.copy()\n",
        "for i in range(len(main_rel_matrix1)):\n",
        "  main_rel_matrix1[i,1] = (main_rel_matrix1[i,1] + rel_size//2) % rel_size\n",
        "\n",
        "main_radj1 = np.array(sorted(main_radj.copy(),key=lambda x: x[1]*10e10+x[0]*10e5))\n",
        "for i in range(len(main_radj1)):\n",
        "  _x,_y = main_radj1[i,0],main_radj1[i,1]\n",
        "  main_radj1[i,0] = _y\n",
        "  main_radj1[i,1] = _x\n",
        "  main_radj1[i,2] = main_radj1[i,2] #( + rel_size//2) % rel_size\n",
        "single_triple_set = set()\n",
        "degree = np.zeros([adj_features.shape[0],3])\n",
        "for triple in main_radj:\n",
        "    single_triple_set.add((triple[0],triple[1]))\n",
        "    if triple[2] < rel_size //2:\n",
        "        degree[triple[0],0] += 1\n",
        "        degree[triple[1],0] += 1\n",
        "        degree[triple[0],2] += 1\n",
        "        degree[triple[1],1] += 1\n",
        "\n",
        "dev_pair_degrees = np.zeros([2,len(dev_pair),3])\n",
        "for i in range(len(dev_pair)):\n",
        "    dev_pair_degrees[0,i,0] = degree[dev_pair[i][0],0]\n",
        "    dev_pair_degrees[0,i,1] = degree[dev_pair[i][0],1]\n",
        "    dev_pair_degrees[0,i,2] = degree[dev_pair[i][0],2]\n",
        "    dev_pair_degrees[1,i,0] = degree[dev_pair[i][1],0]\n",
        "    dev_pair_degrees[1,i,1] = degree[dev_pair[i][1],1]\n",
        "    dev_pair_degrees[1,i,2] = degree[dev_pair[i][1],2]\n",
        "\n",
        "train_pair_degrees = np.zeros([2,len(train_pair),3])\n",
        "for i in range(len(train_pair)):\n",
        "    train_pair_degrees[0,i,0] = degree[train_pair[i][0],0]\n",
        "    train_pair_degrees[0,i,1] = degree[train_pair[i][0],1]\n",
        "    train_pair_degrees[0,i,2] = degree[train_pair[i][0],2]\n",
        "    train_pair_degrees[1,i,0] = degree[train_pair[i][1],0]\n",
        "    train_pair_degrees[1,i,1] = degree[train_pair[i][1],1]\n",
        "    train_pair_degrees[1,i,2] = degree[train_pair[i][1],2]\n",
        "\n",
        "ent_size = adj_features.shape[0]\n",
        "depth = 3\n",
        "deg = np.zeros([ent_size])\n",
        "deg_in = np.zeros([ent_size])\n",
        "deg_out = np.zeros([ent_size])\n",
        "bdeg = np.zeros([ent_size])\n",
        "\n",
        "\n",
        "single_graph = dict()\n",
        "for h,t,r in main_radj:\n",
        "    deg[h] += 1\n",
        "    deg[t] += 1\n",
        "    deg_in[t] += 1\n",
        "    deg_out[h] += 1\n",
        "    if(h not in single_graph):\n",
        "        single_graph[h] = [t]\n",
        "    else:\n",
        "        single_graph[h] += [t]\n",
        "for i in range(ent_size):\n",
        "    bdeg[i] = math.floor(math.log(i+1))\n",
        "\n",
        "bin = int(max(bdeg))\n",
        "rec_deg = np.zeros([ent_size,bin*depth])\n",
        "for h in single_graph:\n",
        "    set_len = len(single_graph[h])\n",
        "    for i in single_graph[h]:\n",
        "        rec_deg[h,int(bdeg[i])] += 1/set_len\n",
        "for d in range(1,depth):\n",
        "    \n",
        "    for h in single_graph:\n",
        "        set_len = len(single_graph[h])\n",
        "        for i in single_graph[h]:\n",
        "            rec_deg[h,d*bin:(d+1)*bin] += (rec_deg[i,(d-1)*bin:(d)*bin])/set_len\n"
      ],
      "metadata": {
        "id": "2-SeSZ4Gvppc"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity1, rel1, triples1 = load_triples('/content/GCN-Align/data/%s_en/'%lang + 'triples_1')\n",
        "num_entity_1 = len(entity1)\n",
        "num_rel_1 = len(rel1)"
      ],
      "metadata": {
        "id": "qjqNt7oeBXjK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = SourceFileLoader(\"layer\", \"/content/RREA/CIKM/layer.py\").load_module()\n",
        "\n",
        "from layer import NR_GraphAttention\n",
        "tf.keras.backend.clear_session()\n",
        "radj = main_radj\n",
        "radj1 = main_radj1\n",
        "rel_matrix = main_rel_matrix\n",
        "rel_matrix1 = main_rel_matrix1\n",
        "node_size = adj_features.shape[0]\n",
        "rel_size = rel_features.shape[1]\n",
        "triple_size = len(adj_matrix)\n",
        "batch_size = node_size\n",
        "datas = []\n",
        "# e stand for entity ,r stand for relation , E stand for encoder(e), c stand for character embedding , R stand for encoder(r)\n",
        "run_name = 'eER'\n",
        "\n",
        "def getMax(d):\n",
        "    l = list(d.keys())\n",
        "    random.shuffle(l)\n",
        "    m = 0 \n",
        "    max_key = -1;\n",
        "    for k in l : \n",
        "        if d[k] > m :\n",
        "            max_key = k\n",
        "            m = d[k]\n",
        "    if m <= 1:\n",
        "        max_key = -1\n",
        "    return max_key\n",
        "\n",
        "def get_match_relations(tpair , threshold = 50):\n",
        "    rmap_predict = dict()\n",
        "    temp_train_pair_dic = dict()\n",
        "    for i in range(len(tpair)):\n",
        "        temp_train_pair_dic[tpair[i][0]] = tpair[i][1]\n",
        "        temp_train_pair_dic[tpair[i][1]] = tpair[i][0]\n",
        "    triple_dic = dict()\n",
        "    for h,t,r in radj:\n",
        "    \n",
        "        if (h,t) in triple_dic:\n",
        "            triple_dic[(h,t)] += [r]\n",
        "        else:\n",
        "            triple_dic[(h,t)] = [r]\n",
        "    for h,t,r in radj:\n",
        "        if h in temp_train_pair_dic and t in temp_train_pair_dic :\n",
        "            z = np.zeros([rel_size])\n",
        "            if (temp_train_pair_dic[h],temp_train_pair_dic[t]) in triple_dic:\n",
        "                z[triple_dic[(temp_train_pair_dic[h],temp_train_pair_dic[t])]] = 1\n",
        "                if r in rmap_predict :\n",
        "                    rmap_predict[r] += z\n",
        "                else:\n",
        "                    rmap_predict[r] = z\n",
        "    i = 0\n",
        "    matched_dic = dict()\n",
        "    for key in rmap_predict:\n",
        "        v = np.max(rmap_predict[key])\n",
        "        i += 1\n",
        "        if(v>threshold):\n",
        "            a = np.argmax(rmap_predict[key]) % (rel_size//2)\n",
        "            matched_dic[key] = a\n",
        "    #print(matched_dic);\n",
        "    return matched_dic\n",
        "\n",
        "def complete_graph(tpair,threshold=2,num = 10):\n",
        "    \n",
        "    new_train_pair = tpair.copy()\n",
        "    for i in range(num):\n",
        "        train_pair_dic1 = dict()\n",
        "        for a,b in new_train_pair : \n",
        "            train_pair_dic1[a] = b\n",
        "            train_pair_dic1[b] = a\n",
        "        selected_rmap = get_match_relations(new_train_pair,threshold)\n",
        "\n",
        "        si = dict()\n",
        "        so = dict()\n",
        "\n",
        "        for h,t,r in radj:\n",
        "          if (t,r) not in si:\n",
        "            si[(t,r)] = [h]\n",
        "          else:\n",
        "            si[(t,r)] += [h]\n",
        "\n",
        "          if (h,r) not in so:\n",
        "            so[(h,r)] = [t]\n",
        "          else:\n",
        "            so[(h,r)] += [t]\n",
        "\n",
        "        lmap = dict()\n",
        "        rmap = dict()\n",
        "        tmap = dict()\n",
        "        for h,t,r in radj:\n",
        "            if h in train_pair_dic1 and r in selected_rmap :\n",
        "                if (train_pair_dic1[h],selected_rmap[r]) in so:\n",
        "                    for t1 in so[(train_pair_dic1[h],selected_rmap[r])]:\n",
        "                        if t not in tmap:\n",
        "                            tmap[t] = dict()\n",
        "                        if t1 not in tmap[t]:\n",
        "                            tmap[t][t1] = 1\n",
        "                        else:\n",
        "                            tmap[t][t1] += 1\n",
        "\n",
        "            if t in train_pair_dic1 and r in selected_rmap  :\n",
        "                if (train_pair_dic1[t],selected_rmap[r]) in si:\n",
        "                    for h1 in si[(train_pair_dic1[t],selected_rmap[r])]:\n",
        "                        if h not in tmap:\n",
        "                            tmap[h] = dict()\n",
        "                        if h1 not in tmap[h]:\n",
        "                            tmap[h][h1] = 1\n",
        "                        else:\n",
        "                            tmap[h][h1] += 1\n",
        "        l = []\n",
        "        for a in tmap:\n",
        "            if a < num_entity_1 and a not in train_pair_dic :\n",
        "                z1 = getMax(tmap[a]);\n",
        "                if  z1 in tmap and a == getMax(tmap[z1]):\n",
        "                  train_pair_dic1[a] = z1\n",
        "                  l.append([a,z1 ])\n",
        "      \n",
        "\n",
        "        # for a,b in pairs:\n",
        "        #     if a not in train_pair_dic :\n",
        "        #       if a in tmap:\n",
        "        #           train_pair_dic1[a] = getMax(tmap[a])\n",
        "        #           l.append([a,train_pair_dic1[a] ])\n",
        "        #       elif a in lmap:\n",
        "        #           train_pair_dic1[a] = getMax(lmap[a])\n",
        "        #           l.append([a,train_pair_dic1[a] ])\n",
        "        #       elif a in rmap:\n",
        "        #           train_pair_dic1[a] = getMax(rmap[a])\n",
        "        #           l.append([a,train_pair_dic1[a]])\n",
        "\n",
        "        s = 0\n",
        "        if (len(l) == 0):\n",
        "            return train_pair_dic1;\n",
        "        new_train_pair = np.concatenate([new_train_pair,np.array(l)],axis=0)\n",
        "        # for t in train_pair_dic1:\n",
        "        #     if t not in train_pair_dic and pair_dic[t] == train_pair_dic1[t]:\n",
        "        #         s += 1\n",
        "        # print(s/(len(pair_dic)-len(train_pair_dic)),len(train_pair_dic1)-len(train_pair_dic))\n",
        "    return train_pair_dic1\n",
        "def replce_matched_relations(threshold = 50):\n",
        "    matched_dic = get_match_relations(threshold)\n",
        "    for i in range(len(radj)) :\n",
        "        h = radj[i,0]\n",
        "        t = radj[i,1]\n",
        "        r = radj[i,2]\n",
        "        if r in matched_dic :\n",
        "            radj[i,2] = matched_dic[r]\n",
        "        elif (r - rel_size//2) in matched_dic:\n",
        "            radj[i,2] = matched_dic[(r - rel_size//2)]   + (rel_size//2)\n",
        "\n",
        "    for i in range(len(rel_matrix)) :\n",
        "        h = rel_matrix[i,0]\n",
        "        r = rel_matrix[i,1]\n",
        "        if r in matched_dic :\n",
        "            rel_matrix[i,1] = matched_dic[r]\n",
        "        elif (r - rel_size//2) in matched_dic:\n",
        "            rel_matrix[i,1] = matched_dic[(r - rel_size//2)]   + (rel_size//2)\n",
        "\n",
        "\n",
        "class TokenEmbedding(keras.layers.Embedding):\n",
        "    \"\"\"Embedding layer with weights returned.\"\"\"\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.input_dim, self.output_dim\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.embeddings\n",
        "    \n",
        "def get_embedding(add_rel_feature = False,threshold = 50,depth = 2):\n",
        "    inputs = [adj_matrix,r_index,r_val,rel_matrix,rel_matrix1,ent_matrix,radj,radj1,nadj]\n",
        "    inputs = [np.expand_dims(item,axis=0) for item in inputs]\n",
        "    if add_rel_feature == False:\n",
        "        return get_emb.predict_on_batch(inputs)\n",
        "    else:\n",
        "        selected_rmap = get_match_relations(threshold)\n",
        "        rselected_rmap = dict()\n",
        "        i = 0\n",
        "        for key in selected_rmap:\n",
        "            rselected_rmap[key] = i\n",
        "            rselected_rmap[selected_rmap[key]] = i\n",
        "            i += 1\n",
        "        f = np.zeros([node_size,len(selected_rmap)*depth])\n",
        "        for h,t,r in radj:\n",
        "            if r in rselected_rmap:\n",
        "                f[h,rselected_rmap[r]] += 1./degree[h,2]\n",
        "            \n",
        "        for i in range(depth-1):\n",
        "            for h,t,r in radj:\n",
        "                f[h,(i+1)*depth:(i+2)*depth] += 1./degree[h,2] * f[t,(i)*depth:(i+1)*depth]\n",
        "\n",
        "        f1 = get_emb.predict_on_batch(inputs)\n",
        "        f1 = f1 / np.linalg.norm(f1,axis=-1,keepdims=True)\n",
        "        f = f / np.linalg.norm(f,axis=-1,keepdims=True)\n",
        "        f2 = np.concatenate([f,f1],axis=1)\n",
        "        return f\n",
        "\n",
        "def test(wrank = None):\n",
        "    vec = get_embedding()\n",
        "    return  get_hits(vec,dev_pair,wrank=wrank)\n",
        "\n",
        "def CSLS_test(thread_number = 16, csls=10,accurate = True):\n",
        "    vec = get_embedding()\n",
        "    #vec =get_embedding(True,threshold = 2,depth = 1)\n",
        "    trest_set_1 = [e1 for e1, e2 in dev_pair]\n",
        "    trest_set_2 = [e2 for e1, e2 in dev_pair]\n",
        "    Lvec = np.array([vec[e1] for e1, e2 in dev_pair])\n",
        "    Rvec = np.array([vec[e2] for e1, e2 in dev_pair])\n",
        "\n",
        "    Lvec = Lvec / np.linalg.norm(Lvec,axis=-1,keepdims=True)\n",
        "    Rvec = Rvec / np.linalg.norm(Rvec,axis=-1,keepdims=True)\n",
        "\n",
        "    # Lvec1 = np.array([rec_deg[e1] for e1, e2 in dev_pair])\n",
        "    # Rvec1 = np.array([rec_deg[e2] for e1, e2 in dev_pair])\n",
        "\n",
        "    # Lvec1 = 0.5 * Lvec1 / np.linalg.norm(Lvec1,axis=-1,keepdims=True)\n",
        "    # Rvec1 = 0.5 * Rvec1 / np.linalg.norm(Rvec1,axis=-1,keepdims=True)\n",
        "\n",
        "    # Lvec = np.concatenate([Lvec,Lvec1],axis=-1)\n",
        "    # Rvec = np.concatenate([Rvec,Rvec1],axis=-1)\n",
        "\n",
        "    # Lvec = Lvec / np.linalg.norm(Lvec,axis=-1,keepdims=True)\n",
        "    # Rvec = Rvec / np.linalg.norm(Rvec,axis=-1,keepdims=True)\n",
        "\n",
        "    lprec,ldata,_ = eval_alignment_by_sim_mat(Lvec, Rvec, [1, 5, 10], thread_number, csls=csls, accurate=accurate)\n",
        "    results = np.zeros([len(dev_pair)])\n",
        "\n",
        "    for i in range(len(dev_pair)):\n",
        "        if((i,i) in lprec):\n",
        "            results[i] = 1\n",
        "\n",
        "    datas.append((results,ldata))\n",
        "    save_datas()\n",
        "    return None\n",
        "\n",
        "def save_datas():\n",
        "    with open('/content/RREA/'+lang+\"_\"+run_name+'.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "        pickle.dump([train_pair_degrees,dev_pair_degrees,datas], f)\n",
        "\n",
        "def load_datas():\n",
        "    with open('/content/RREA/'+lang+\"_\"+run_name+'.pkl', 'rb') as f:  # Python 3: open(..., 'wb')\n",
        "        mydata =pickle.load(f)\n",
        "    return mydata;\n",
        "\n",
        "def get_train_set(batch_size = batch_size):\n",
        "    negative_ratio =  batch_size // len(train_pair) + 1\n",
        "    train_set = np.reshape(np.repeat(np.expand_dims(train_pair,axis=0),axis=0,repeats=negative_ratio),newshape=(-1,2))\n",
        "    np.random.shuffle(train_set); train_set = train_set[:batch_size]\n",
        "    train_set = np.concatenate([train_set,np.random.randint(0,node_size,[train_set.shape[0],4])],axis = -1)\n",
        "    return train_set\n",
        "\n",
        "def get_train_set1(batch_size = batch_size):\n",
        "    train_set = train_pair\n",
        "    np.random.shuffle(train_set);\n",
        "    train_set = np.concatenate([train_set,np.random.randint(0,node_size,train_set.shape)],axis = -1)\n",
        "    return train_set\n",
        "\n",
        "def get_trgat(node_size,rel_size,node_hidden,rel_hidden,triple_size,n_attn_heads = 2,dropout_rate = 0,gamma = 3,lr = 0.005,depth = 2):\n",
        "    adj_input = Input(shape=(None,2))\n",
        "    index_input = Input(shape=(None,2),dtype='int64')\n",
        "    val_input = Input(shape = (None,))\n",
        "    rel_adj = Input(shape=(None,2))\n",
        "    rel_adj1 = Input(shape=(None,2))\n",
        "    ent_adj = Input(shape=(None,2))\n",
        "    radj = Input(shape=(None,3),dtype='int64')\n",
        "    radj1 = Input(shape=(None,3),dtype='int64')\n",
        "    nadj = Input(shape=(None,3))\n",
        "    \n",
        "    # ent_emb = TokenEmbedding(node_size,node_hidden,embeddings_initializer=\"LecunNormal\",trainable = True)(val_input) \n",
        "    # rel_emb = TokenEmbedding(rel_size,node_hidden,embeddings_initializer=\"LecunNormal\",trainable = True)(val_input)\n",
        "    # ch_emb = TokenEmbedding(char_size,node_hidden,embeddings_initializer=\"LecunNormal\",trainable = True)(val_input)\n",
        "\n",
        "    ent_emb = TokenEmbedding(node_size,node_hidden,trainable = True)(val_input) \n",
        "    rel_emb = TokenEmbedding(rel_size,node_hidden,trainable = True)(val_input)\n",
        "    ch_emb = TokenEmbedding(char_size,node_hidden,trainable = True)(val_input)\n",
        "\n",
        "\n",
        "    # E = TokenEmbedding(node_hidden,node_hidden,trainable = True)(val_input)\n",
        "    # R = TokenEmbedding(node_hidden,node_hidden,trainable = True)(val_input)\n",
        "    def ch_avg(tensor,size):\n",
        "        n_adj = tf.squeeze(tensor[0],axis = 0)\n",
        "        adj = K.cast(n_adj[:,0:2],dtype = \"int64\")   \n",
        "        adj = tf.SparseTensor(indices=adj, values=tf.ones_like(n_adj[:,2],dtype = 'float32') , dense_shape=(node_size,size)) \n",
        "        adj = tf.compat.v1.sparse_softmax(adj) \n",
        "        l_adj = tf.compat.v1.sparse_tensor_dense_matmul(adj,tensor[1])\n",
        "        return l_adj\n",
        "\n",
        "    def avg(tensor,size):\n",
        "        adj = K.cast(K.squeeze(tensor[0],axis = 0),dtype = \"int64\")   \n",
        "        adj = tf.SparseTensor(indices=adj, values=tf.ones_like(adj[:,0],dtype = 'float32'), dense_shape=(node_size,size)) \n",
        "        adj = tf.compat.v1.sparse_softmax(adj) \n",
        "        l_adj = tf.compat.v1.sparse_tensor_dense_matmul(adj,tensor[1])\n",
        "        return l_adj\n",
        "\n",
        "    name_feature = Lambda(ch_avg,arguments={'size':char_size})([nadj,ch_emb])\n",
        "    # rel_feature1 = Lambda(avg,arguments={'size':rel_size//2})([rel_adj,rel_emb])\n",
        "    # rel_feature2 = Lambda(avg,arguments={'size':rel_size//2})([rel_adj1,rel_emb])\n",
        "    rel_feature = Lambda(avg,arguments={'size':rel_size})([rel_adj,rel_emb])\n",
        "    # rel_feature1= Lambda(avg,arguments={'size':node_size})([rel_adj,rel_feature])\n",
        "    # rel_feature = 0.8 * rel_feature + 0.2 * rel_feature1\n",
        "\n",
        "    ent_feature = Lambda(avg,arguments={'size':node_size})([ent_adj,ent_emb])\n",
        "    ent_feature1 = Lambda(avg,arguments={'size':node_size})([ent_adj,ent_feature])\n",
        "    ent_feature = 0.8 * ent_feature + 0.2 * ent_feature1\n",
        "    # rel_emb = K.concatenate([rel_emb1,rel_emb2])\n",
        "    #rel_feature = K.concatenate([rel_feature1,rel_feature2])\n",
        "\n",
        "    encoder = NR_GraphAttention(node_size,activation=\"relu\",\n",
        "                                       rel_size = rel_size,\n",
        "                                       depth =2,\n",
        "                                       attn_heads=n_attn_heads,\n",
        "                                       triple_size = triple_size,\n",
        "                                       attn_heads_reduction='average',   \n",
        "                                       dropout_rate=dropout_rate)\n",
        "    encoder1 = NR_GraphAttention(node_size,activation=\"relu\",\n",
        "                                    rel_size = rel_size,\n",
        "                                    depth = 2,\n",
        "                                    attn_heads=n_attn_heads,\n",
        "                                    triple_size = triple_size,\n",
        "                                    attn_heads_reduction='average',   \n",
        "                                    dropout_rate=dropout_rate)\n",
        "    encoder2 = NR_GraphAttention(node_size,activation=\"relu\",\n",
        "                                       rel_size = rel_size,\n",
        "                                       depth = 2,\n",
        "                                       attn_heads=n_attn_heads,\n",
        "                                       triple_size = triple_size,\n",
        "                                       attn_heads_reduction='average',   \n",
        "                                       dropout_rate=dropout_rate)\n",
        "    opt1 = [rel_emb,adj_input,index_input,val_input,radj]\n",
        "    opt2 = [rel_emb,adj_input,index_input,val_input,radj1]\n",
        "    elements = []\n",
        "    coefs = []\n",
        "    for f_name in run_name:\n",
        "        if f_name == 'e':\n",
        "            elements += [ent_emb]\n",
        "            coefs += [0.2]\n",
        "        elif f_name == 'E':\n",
        "            elements += encoder1([ent_feature]+opt1) \n",
        "            coefs += [0.8,0.8,0.8]\n",
        "        elif f_name == 'R':\n",
        "            elements += encoder1([rel_feature]+opt1) #+ encoder([rel_feature1]+opt2)\n",
        "            coefs += [0.8,0.8,0.8]\n",
        "        elif f_name == 'q':\n",
        "            elements += [rel_feature1]\n",
        "            coefs +=  [-0.5]\n",
        "        elif f_name == 'c':\n",
        "            elements += [name_feature]\n",
        "            coefs += [1.]\n",
        "\n",
        "    #elements = [ent_emb] +   encoder([rel_feature]+opt1)+encoder([ent_feature]+opt1)\n",
        "    #elements = [0.3 * ent_emb[:,:node_hidden//2]]+ el[0:3]+el[6:9]+[0.2 * ent_emb[:,node_hidden//2:]] + el[3:6]+el[9:12]\n",
        "    num_el = elements.__len__()\n",
        "    alpha =  tf.Variable([0.2], trainable=False)\n",
        "    weight =  tf.Variable(coefs, trainable=False)\n",
        "    #weight = (num_el-1)* tf.math.softmax(weight)\n",
        "    #weight =  tf.Variable(1 * [-0.2,1.0] + (num_el-2) * [.8], trainable=False)\n",
        "    print(weight)\n",
        "    #elements = [alpha * elements[0]] + [elements[_i+1]*(1-alpha) for _i in range(len(elements)-1) ]\n",
        "    elements = [elements[_i]*weight[_i] for _i in range(len(elements)) ]\n",
        "    out_feature = Concatenate(-1)(elements)\n",
        "    out_feature = Dropout(dropout_rate)(out_feature)\n",
        "    \n",
        "    alignment_input = Input(shape=(None,6),dtype = \"int64\") \n",
        "    #find = Lambda(lambda x:K.gather(reference=x[0],indices=K.cast(K.squeeze(x[1],axis=0), 'int32')))([out_feature,alignment_input])\n",
        "\n",
        "    I = K.cast(K.squeeze(alignment_input,axis=0),dtype = \"int64\")\n",
        "   \n",
        "    A = K.sum(K.abs(K.gather(out_feature,I[:,0])-K.gather(out_feature,I[:,1])),axis=-1,keepdims=True)\n",
        "    loss = 0.1 * ( K.relu(1 - K.sum(K.abs(K.gather(out_feature,I[:,0])))) +K.relu(1 - K.sum(K.abs(K.gather(out_feature,I[:,1])))))\n",
        "    for i in range(2):\n",
        "        B = K.sum(K.abs(K.gather(out_feature,I[:,0])-K.gather(out_feature,I[:,2+i])),axis=-1,keepdims=True)\n",
        "        C = K.sum(K.abs(K.gather(out_feature,I[:,1])-K.gather(out_feature,I[:,2+i])),axis=-1,keepdims=True)\n",
        "        loss += K.relu(gamma +A -B) + K.relu(gamma +A -C)\n",
        "        loss += 0.1 * ( K.relu(1 - K.sum(K.abs(K.gather(out_feature,I[:,2+i])))))\n",
        "    loss =  tf.compat.v1.reduce_sum(loss,keep_dims=True) / (batch_size)    \n",
        "   \n",
        "    inputs = [adj_input,index_input,val_input,rel_adj,rel_adj1,ent_adj,radj,radj1,nadj]\n",
        "    train_model = keras.Model(inputs = inputs + [alignment_input],outputs = loss)\n",
        "    \n",
        "    train_model.compile(loss=lambda y_true,y_pred: y_pred,optimizer=tf.keras.optimizers.RMSprop(lr=lr))\n",
        "    \n",
        "    feature_model = keras.Model(inputs = inputs,outputs = out_feature)\n",
        "    return train_model,feature_model"
      ],
      "metadata": {
        "id": "OwozXDxFgycr"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = 'eER'\n",
        "model,get_emb = get_trgat(dropout_rate=0.3,node_size=node_size,rel_size=rel_size,n_attn_heads = 1,depth=2,gamma =3,node_hidden=100,rel_hidden = 100,triple_size = triple_size)\n",
        "#model.summary(); \n",
        "initial_weights = model.get_weights()"
      ],
      "metadata": {
        "id": "VOZDkwnYjLrL",
        "outputId": "a3ad7f93-d99d-4f8a-ac6d-e61469516d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(7,) dtype=float32, numpy=array([0.2, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pair = train_pair_main\n",
        "\n",
        "train_pair_dic = dict()\n",
        "for i in range(len(train_pair)):\n",
        "    train_pair_dic[train_pair[i][0]] = train_pair[i][1]\n",
        "    \n",
        "radj = main_radj\n",
        "radj1 = main_radj1\n",
        "rel_matrix = main_rel_matrix\n",
        "rel_matrix1 = main_rel_matrix1\n",
        "tf.keras.backend.clear_session()\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True  \n",
        "sess = tf.compat.v1.Session(config=config) \n",
        "\n",
        "rest_set_1 = [e1 for e1, e2 in dev_pair]\n",
        "rest_set_2 = [e2 for e1, e2 in dev_pair]\n",
        "np.random.shuffle(rest_set_1)\n",
        "np.random.shuffle(rest_set_2)\n",
        "\n",
        "epoch = 100\n",
        "for turn in range(20):\n",
        "    print(\"iteration %d start.\"%turn)\n",
        "    for i in trange(epoch):\n",
        "        cur_epoch = i\n",
        "        train_set = get_train_set()\n",
        "        \n",
        "        inputs = [adj_matrix,r_index,r_val,rel_matrix,rel_matrix1,ent_matrix,radj,radj1,nadj,train_set]\n",
        "        inputs = [np.expand_dims(item,axis=0) for item in inputs]\n",
        "        model.train_on_batch(inputs,np.zeros((1,1)))\n",
        "        #CSLS_test()\n",
        "        if i%100 == 99 and i+1 != epoch:\n",
        "            \n",
        "            CSLS_test()     \n",
        "    \n",
        "    if turn == 1:\n",
        "        epoch = 100\n",
        "        CSLS_test()\n",
        "        break;\n",
        "    CSLS_test()       \n",
        "    new_pair1 = []\n",
        "    new_pair = []\n",
        "    \n",
        "    #vec = get_embedding()\n",
        "    vec = get_embedding()\n",
        "    Lvec = np.array([vec[e] for e in rest_set_1])\n",
        "    Rvec = np.array([vec[e] for e in rest_set_2])\n",
        "    Lvec = Lvec / np.linalg.norm(Lvec,axis=-1,keepdims=True)\n",
        "    Rvec = Rvec / np.linalg.norm(Rvec,axis=-1,keepdims=True)\n",
        "    A,_,_ = eval_alignment_by_sim_mat(Lvec, Rvec, [1, 5, 10], 16,10,True,False)\n",
        "    B,_,_ = eval_alignment_by_sim_mat(Rvec, Lvec,[1, 5, 10], 16,10,True,False)\n",
        "    A = sorted(list(A)); B = sorted(list(B))\n",
        "    AB = dict()\n",
        "    for a,b in A:\n",
        "        if  B[b][1] == a:\n",
        "            if rest_set_1[a] not in AB:\n",
        "                AB[rest_set_1[a]] = 1\n",
        "            else:\n",
        "                AB[rest_set_1[a]] += 1\n",
        "            if rest_set_2[b] not in AB:\n",
        "                AB[rest_set_2[b]] = 1\n",
        "            else:\n",
        "                AB[rest_set_2[b]] += 1\n",
        "    for a,b in A:\n",
        "        if  B[b][1] == a  and AB[rest_set_1[a]] == 1 and AB[rest_set_2[b]] == 1 :\n",
        "            new_pair1.append([rest_set_1[a],rest_set_2[b]])\n",
        "    if turn < 10 :\n",
        "        train_pair1 = np.concatenate([train_pair,np.array(new_pair1)],axis = 0)\n",
        "        new_pair_dic = complete_graph(train_pair1,2,5) \n",
        "        \n",
        "        for a,b in new_pair1:\n",
        "            \n",
        "              if (a in new_pair_dic and new_pair_dic[a] == b):\n",
        "                  new_pair.append([a,b])\n",
        "    else:\n",
        "        new_pair = new_pair1\n",
        "\n",
        "\n",
        "    # _,_,A = eval_alignment_by_sim_mat(Lvec, Rvec, [1, 5, 10], 16,10,True,False)\n",
        "    # _,_,B = eval_alignment_by_sim_mat(Rvec, Lvec,[1, 5, 10], 16,10,True,False)\n",
        "    # A = A[0]\n",
        "    # B = B[0]\n",
        "\n",
        "    # for i in range(len(A)):\n",
        "    #     if i == B[A[i,0],0]:\n",
        "    #         has_other_pair = 0\n",
        "    #         for _i in range(1,10):\n",
        "    #             if(_i in B[A[i,_i],:10]):\n",
        "    #                 has_other_pair = 1\n",
        "    #                 break;\n",
        "\n",
        "    #         if has_other_pair == 0:\n",
        "    #               #print([rest_set_1[i],rest_set_2[A[i,j]]])\n",
        "    #               new_pair.append([rest_set_1[i],rest_set_2[A[i,0]]])\n",
        "\n",
        "\n",
        "    # new_pair_dic = complete_graph(train_pair,2,10) \n",
        "    # for a,b in new_pair1:\n",
        "    #     if (a in new_pair_dic and new_pair_dic[a] == b):\n",
        "    #         new_pair.append([a,b])\n",
        "\n",
        "    train_pair = np.concatenate([train_pair,np.array(new_pair)],axis = 0)\n",
        "    # s = 0\n",
        "    # s1 = 0\n",
        "    # for a,b in new_pair :\n",
        "    #     if pair_dic[a] == b:\n",
        "    #         s1 += 1\n",
        "    # for a,b in train_pair:\n",
        "    #     if a not in train_pair_dic and pair_dic[a] == b:\n",
        "    #         s += 1\n",
        "\n",
        "    # print(\"generate new semi-pairs: \" + str(len(new_pair)) + \".  num of correct new pairs\" + str(s1))\n",
        "    # print(\"total new pairs: \" + str(len(train_pair) - len(train_pair_dic)) + \".  num of correct new pairs \" + str(s) +  \"(\"+str(s/(len(train_pair) - len(train_pair_dic))) +\")\")\n",
        "    for e1,e2 in new_pair:\n",
        "        if e1 in rest_set_1:\n",
        "            rest_set_1.remove(e1) \n",
        "        \n",
        "    for e1,e2 in new_pair:\n",
        "        if e2 in rest_set_2:\n",
        "            rest_set_2.remove(e2)\n",
        "            \n",
        "    with open('/content/RREA/'+lang+\"_\"+run_name+'_train_pair'+str(turn)+'.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
        "        pickle.dump(train_pair, f)\n",
        "\n",
        "    #complete_graph(train_pair,2,10)\n",
        "    #match_relations()\n",
        "# for turn in range(5):\n",
        "#     files.download('/content/RREA/'+lang+\"_\"+run_name+'_train_pair'+str(turn)+'.pkl');\n",
        "# files.download('/content/RREA/'+lang+\"_\"+run_name+'.pkl') "
      ],
      "metadata": {
        "id": "pLYXVCI1iB4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffd4cfa-306c-4c22-a537-7c24499b27da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 start.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|â–ˆ         | 10/100 [00:11<01:05,  1.37it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_pair_dic = complete_graph(train_pair,2,5) "
      ],
      "metadata": {
        "id": "ctjZUoWDqKee"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_pair_dic)"
      ],
      "metadata": {
        "id": "GJQ5TmkYufie",
        "outputId": "f88db6be-c5e6-46e3-ea2a-eb6cc430bf73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15245"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    train_pair_dic1 = dict()\n",
        "    for a,b in new_train_pair : \n",
        "        train_pair_dic1[a] = b\n",
        "        train_pair_dic1[b] = a\n",
        "    selected_rmap = get_match_relations(new_train_pair,threshold)\n",
        "\n",
        "    si = dict()\n",
        "    so = dict()\n",
        "\n",
        "    for h,t,r in radj:\n",
        "      if (t,r) not in si:\n",
        "        si[(t,r)] = [h]\n",
        "      else:\n",
        "        si[(t,r)] += [h]\n",
        "\n",
        "      if (h,r) not in so:\n",
        "        so[(h,r)] = [t]\n",
        "      else:\n",
        "        so[(h,r)] += [t]"
      ],
      "metadata": {
        "id": "IFrIgyveqPC0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    i=0\n",
        "    lmap = dict()\n",
        "    rmap = dict()\n",
        "    tmap = dict()\n",
        "    for h,t,r in radj:\n",
        "        if h in train_pair_dic1 and r in selected_rmap :\n",
        "            if (train_pair_dic1[h],selected_rmap[r]) in so:\n",
        "                for t1 in so[(train_pair_dic1[h],selected_rmap[r])]:\n",
        "                    if t not in tmap:\n",
        "                        tmap[t] = dict()\n",
        "                    if t1 not in tmap[t]:\n",
        "                        tmap[t][t1] = 1\n",
        "                    else:\n",
        "                        tmap[t][t1] += 1\n",
        "\n",
        "        if t in train_pair_dic1 and r in selected_rmap  :\n",
        "            if (train_pair_dic1[t],selected_rmap[r]) in si:\n",
        "                for h1 in si[(train_pair_dic1[t],selected_rmap[r])]:\n",
        "                    if h not in tmap:\n",
        "                        tmap[h] = dict()\n",
        "                    if h1 not in tmap[h]:\n",
        "                        tmap[h][h1] = 1\n",
        "                    else:\n",
        "                        tmap[h][h1] += 1"
      ],
      "metadata": {
        "id": "3EfhQtwIqSIc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for a in tmap:\n",
        "    if a < num_entity_1 and a not in train_pair_dic :\n",
        "        z1 = getMax(tmap[a]);\n",
        "        if  z1 in tmap and a == getMax(tmap[z1]):\n",
        "          train_pair_dic1[a] = z1\n",
        "          l.append([a,z1 ])\n",
        "print(l)\n",
        "print(len(l))"
      ],
      "metadata": {
        "id": "jMFAL6A3qbgl",
        "outputId": "cf338b68-b540-413f-8a2e-c21e8e594f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 10506], [4728, 15228], [499, 10999], [17, 10517], [2268, 12768], [3355, 13855], [2096, 12596], [31, 10531], [243, 10743], [5031, 15531], [10157, 20657], [42, 10542], [3738, 14238], [4756, 15256], [6624, 17124], [3892, 14392], [53, 19492], [56, 10556], [7840, 18340], [65, 10565], [897, 11397], [73, 10573], [79, 10579], [1511, 12011], [4390, 14890], [1600, 12100], [6807, 17307], [7890, 18390], [6086, 16586], [5528, 16028], [10245, 20745], [100, 10600], [9097, 19597], [102, 10602], [103, 10603], [104, 14904], [112, 10612], [117, 10617], [9187, 19687], [126, 10626], [131, 10631], [1780, 12280], [133, 10633], [141, 10641], [4457, 14957], [444, 10944], [1573, 12073], [6784, 17284], [3195, 13695], [9253, 19753], [157, 14257], [163, 10663], [165, 10665], [172, 10672], [7118, 17618], [179, 10679], [186, 10686], [1578, 12078], [7250, 17750], [196, 37641], [7800, 18300], [9145, 19645], [10493, 20993], [209, 10709], [212, 10712], [923, 11423], [4088, 14588], [6604, 17104], [9829, 20329], [4735, 15235], [219, 10719], [222, 10722], [223, 10723], [224, 10724], [856, 11356], [1323, 11199], [228, 11439], [232, 10732], [10178, 20678], [724, 11224], [7366, 17866], [8757, 19257], [248, 10748], [253, 10753], [7317, 17817], [265, 10765], [6349, 16849], [270, 10770], [2308, 12808], [7594, 36914], [3591, 14091], [282, 10782], [284, 10784], [685, 11185], [2388, 12888], [8646, 19146], [5572, 16072], [289, 10789], [290, 10790], [291, 10791], [4507, 15007], [9877, 20377], [7316, 17816], [305, 10805], [306, 10806], [6792, 17292], [315, 10815], [323, 11936], [326, 35202], [330, 10830], [331, 10831], [332, 10832], [4282, 14782], [6618, 17118], [8454, 18954], [368, 10868], [373, 10873], [375, 10875], [9072, 19572], [377, 34525], [5784, 16284], [381, 10881], [9661, 20161], [396, 10896], [8162, 18662], [408, 10908], [1681, 12181], [5764, 16264], [6310, 16810], [8462, 18962], [415, 10915], [3022, 13522], [10085, 20585], [1504, 12004], [2742, 13242], [5497, 15997], [7044, 17544], [7175, 17675], [8611, 19111], [424, 10808], [10242, 20742], [9756, 20256], [8145, 18645], [5314, 15814], [4862, 15362], [7205, 17705], [463, 10963], [1147, 11647], [9720, 20220], [1297, 11797], [9723, 20223], [6854, 17354], [489, 10989], [4266, 14766], [498, 10998], [500, 11000], [501, 11001], [506, 11006], [508, 11008], [512, 11012], [8498, 18998], [518, 11018], [8480, 18980], [9019, 35458], [6991, 17491], [3600, 14100], [10409, 20909], [536, 11036], [541, 11041], [6748, 17248], [4336, 14836], [564, 11064], [4364, 14864], [577, 11077], [581, 11081], [589, 11089], [590, 11090], [600, 11100], [602, 11102], [608, 11108], [611, 11111], [4585, 15085], [615, 11115], [622, 11122], [948, 11448], [4270, 14770], [4886, 15386], [9281, 19974], [628, 11128], [8061, 18561], [6014, 16514], [3041, 13541], [8321, 18821], [647, 16039], [656, 11156], [659, 11159], [661, 10735], [663, 11163], [6050, 16550], [9695, 20195], [10103, 20603], [5468, 15968], [686, 11186], [9245, 19745], [691, 11191], [701, 11201], [4152, 14652], [6842, 17342], [706, 11206], [709, 11209], [730, 11230], [733, 11233], [734, 11234], [9768, 20268], [737, 11237], [1400, 11900], [1423, 11923], [3751, 14251], [759, 11259], [760, 11260], [773, 11273], [2061, 12561], [7107, 17607], [784, 11284], [943, 11443], [1609, 12109], [7706, 20966], [792, 11292], [1045, 11545], [5992, 16492], [2224, 12724], [814, 11314], [815, 11315], [817, 11317], [9922, 20422], [825, 11325], [1607, 12107], [827, 11327], [3584, 14084], [3887, 14387], [843, 11343], [853, 11353], [6297, 16797], [10018, 20518], [858, 11358], [866, 11366], [871, 11371], [873, 11373], [880, 11380], [5443, 15943], [904, 11404], [905, 11405], [910, 11410], [5499, 15999], [7865, 18365], [914, 11414], [919, 11419], [4757, 15257], [5804, 16304], [921, 11421], [6172, 16672], [8005, 18505], [8839, 19339], [952, 11452], [4955, 15455], [962, 11462], [968, 11468], [970, 11470], [3705, 14205], [4633, 15133], [975, 11475], [4592, 15092], [980, 11480], [7590, 18090], [6016, 16516], [10401, 20901], [992, 11492], [1061, 11561], [3737, 14237], [6739, 17239], [1021, 11521], [1027, 11527], [1030, 11530], [5765, 16265], [1037, 11537], [1040, 11540], [1043, 11543], [5002, 15502], [1695, 12195], [2385, 12885], [1068, 11568], [1077, 11577], [1079, 11579], [1084, 11584], [1098, 11598], [1101, 11601], [3652, 14152], [7990, 18490], [10353, 20853], [1703, 12203], [7730, 18230], [2218, 12718], [4020, 14520], [6596, 17096], [7241, 17741], [8351, 18851], [8396, 18896], [1114, 11614], [9974, 20474], [1118, 11618], [1126, 11626], [6481, 16981], [4077, 14577], [4843, 15343], [4972, 15472], [6308, 16808], [1588, 12088], [1144, 14220], [1148, 11648], [6353, 16853], [1154, 11654], [435, 11296], [2310, 12810], [6171, 16671], [6558, 17058], [6828, 17328], [8068, 18568], [5116, 15616], [1169, 11669], [1174, 11674], [2069, 12569], [1177, 11677], [1179, 11679], [1183, 11683], [1189, 11689], [1195, 11695], [6021, 16521], [6661, 17161], [1204, 38347], [7392, 17892], [5111, 15611], [1213, 11713], [6173, 16673], [9519, 20019], [1223, 11723], [1224, 11724], [6409, 16909], [5277, 15777], [1236, 11736], [1254, 11754], [1263, 11763], [1265, 11765], [1696, 12196], [3793, 14293], [1275, 11775], [7750, 18250], [2270, 12770], [5789, 16289], [2912, 13412], [6948, 17448], [1300, 11800], [1307, 11807], [1308, 36525], [3599, 14099], [7337, 17837], [1321, 11821], [1325, 11825], [2729, 13229], [3762, 14262], [7565, 18065], [2803, 37952], [1365, 11865], [4954, 15454], [2767, 13267], [1389, 17702], [1391, 11891], [1402, 11902], [1408, 11908], [1410, 11910], [1414, 11914], [1427, 14266], [5852, 16352], [4350, 14850], [8384, 18884], [1450, 11950], [7214, 17714], [1466, 11966], [5178, 15678], [5770, 16270], [9310, 19810], [9207, 19707], [2758, 13258], [1481, 11981], [1484, 11984], [1488, 11988], [7585, 18085], [8737, 19237], [1493, 11993], [7729, 18229], [10148, 20648], [1502, 12002], [1510, 12010], [1513, 12013], [1518, 12018], [3388, 13888], [8117, 18617], [7110, 17610], [1535, 12035], [1537, 12037], [5668, 16168], [7342, 17842], [8634, 19134], [9071, 19571], [9592, 20092], [1654, 12154], [1685, 12185], [5041, 15541], [4272, 14772], [1559, 12059], [1562, 12062], [1567, 12067], [1825, 12325], [5556, 16056], [1575, 12075], [4064, 14564], [7596, 18096], [1590, 12090], [1593, 12093], [1601, 12101], [8691, 19191], [8938, 19438], [1610, 12110], [1617, 12117], [1618, 16003], [2660, 13160], [8106, 18606], [1629, 12129], [1632, 12132], [1634, 12134], [1643, 12143], [3373, 13873], [6176, 16676], [4874, 15374], [1735, 12235], [1657, 12157], [1664, 12164], [9689, 20189], [1666, 12166], [1668, 17428], [1669, 12169], [2631, 13131], [1674, 12174], [6634, 17134], [4895, 15395], [5868, 16368], [1688, 12188], [3158, 19801], [1698, 12198], [10041, 20541], [1796, 12296], [6471, 16971], [1713, 12213], [1715, 12215], [1720, 11687], [8799, 19299], [7411, 17911], [1731, 15246], [1738, 37192], [1747, 12247], [1748, 12248], [1751, 12251], [4242, 14742], [1957, 12457], [1770, 13122], [1771, 12271], [3347, 13847], [6777, 17277], [1799, 12299], [1802, 12302], [8579, 19079], [4970, 15470], [1813, 12313], [8753, 19253], [1817, 12317], [1827, 12327], [6132, 16632], [9930, 20430], [1829, 12329], [984, 11484], [2462, 12962], [1012, 11512], [8960, 19460], [7794, 18294], [1854, 12354], [1855, 12355], [2056, 12556], [6507, 17007], [9806, 20306], [9476, 20554], [8221, 18721], [9808, 20308], [4200, 14700], [8300, 18800], [1903, 20984], [1907, 12407], [1909, 12409], [1910, 12410], [2048, 12548], [2219, 12719], [4075, 14575], [10299, 20799], [1920, 12420], [1923, 12376], [6523, 17023], [5663, 16163], [1945, 12445], [9528, 20028], [9761, 20261], [1951, 12451], [1959, 12459], [933, 11433], [7710, 18210], [6975, 17475], [1982, 12482], [1986, 12486], [1988, 12488], [1991, 37438], [1992, 12492], [2836, 13336], [1996, 12496], [2958, 13458], [2017, 12517], [10127, 37301], [75, 10575], [2033, 12533], [5460, 15960], [6039, 16539], [3409, 13909], [2433, 12933], [2053, 12553], [2065, 12565], [5040, 15540], [8092, 18592], [2082, 12582], [4622, 15122], [2091, 12591], [2092, 12592], [2093, 12593], [3721, 14221], [6656, 17156], [2108, 12608], [3729, 14229], [5292, 15792], [4964, 15464], [3536, 14036], [210, 10710], [8881, 19381], [2145, 12645], [2147, 12647], [2148, 12648], [2150, 12650], [5312, 15812], [4961, 12611], [6515, 17015], [2162, 12662], [4639, 15139], [5188, 15688], [7058, 17558], [8047, 18547], [2164, 12664], [5320, 15820], [4568, 16277], [2185, 12685], [6532, 17032], [6836, 17336], [2191, 12691], [7860, 18360], [10020, 20520], [2195, 12695], [2777, 13277], [3467, 13967], [2221, 12721], [2231, 37424], [3763, 14263], [9368, 19868], [2236, 12736], [9838, 20338], [2241, 12741], [2243, 12743], [5578, 16078], [8496, 18996], [3636, 14136], [5887, 16387], [2258, 12758], [2259, 12759], [8088, 18588], [8847, 19347], [2274, 12774], [7471, 17971], [2949, 13449], [8403, 18903], [2279, 12779], [2284, 12784], [7460, 17960], [2610, 13110], [2297, 12797], [5122, 15622], [2908, 13408], [6867, 17367], [3549, 14049], [4302, 14802], [7135, 17635], [3454, 13954], [9809, 20309], [10128, 20628], [6885, 17385], [2366, 12866], [2373, 12873], [2376, 12935], [4439, 14939], [4321, 14821], [10164, 20664], [2390, 12890], [2395, 12895], [3161, 13661], [2405, 12905], [6542, 17042], [2410, 12910], [2952, 13452], [5430, 15930], [8654, 19154], [2417, 12917], [2425, 12925], [2446, 12946], [2450, 12950], [2460, 12960], [2463, 12963], [7972, 18472], [6468, 16968], [2478, 12978], [2479, 12979], [2487, 12987], [2494, 12994], [4901, 15401], [2512, 35521], [9903, 20403], [7681, 18181], [2524, 35144], [2526, 13026], [2528, 13028], [4317, 14817], [5983, 16483], [8404, 18904], [3122, 13622], [2537, 13037], [2788, 13288], [2540, 13040], [2556, 13056], [7090, 17590], [8489, 18989], [2564, 13064], [2580, 13080], [2581, 13081], [2582, 13082], [4824, 15324], [6654, 17154], [7578, 18078], [10211, 20711], [2667, 13167], [6511, 17011], [2595, 13095], [2598, 13098], [7026, 17526], [2602, 13102], [6883, 17383], [9792, 20292], [2604, 13104], [2611, 13111], [2615, 12288], [2797, 13297], [5143, 15643], [2621, 13121], [2636, 13136], [2638, 13138], [7836, 18336], [2668, 12126], [9409, 19909], [2674, 13174], [2679, 13179], [2682, 13182], [3630, 14130], [4092, 14592], [2685, 13185], [2688, 13188], [2690, 13190], [6855, 17355], [2703, 13203], [4003, 14503], [2712, 13212], [2721, 13221], [2724, 14508], [2735, 13235], [2743, 15281], [2746, 13246], [3552, 14052], [2756, 13256], [2772, 13272], [2774, 13274], [3133, 13633], [10163, 20663], [6054, 16554], [2785, 13285], [2786, 13286], [2789, 13289], [2792, 13292], [2799, 13299], [8769, 19269], [3515, 14015], [2858, 13358], [2838, 13338], [2842, 13342], [2852, 13352], [2853, 13353], [6107, 16607], [9182, 19682], [2870, 13370], [2878, 13378], [2892, 13392], [2893, 13393], [6897, 34739], [2900, 13400], [2901, 11359], [3944, 14444], [3799, 14299], [4749, 15249], [7319, 17819], [8652, 19152], [9107, 19607], [2920, 13420], [2927, 36255], [2931, 13431], [5346, 15846], [9897, 20397], [188, 10688], [2950, 13450], [2964, 13464], [5738, 16238], [7979, 18479], [2998, 13498], [3965, 14465], [6617, 17117], [3006, 13506], [3007, 18089], [3012, 13512], [3023, 13523], [9889, 20389], [5465, 15965], [3035, 13535], [3039, 13539], [3047, 13547], [3059, 13559], [3060, 13560], [3062, 13562], [3072, 13572], [5778, 16278], [9439, 19939], [4079, 14579], [9286, 19786], [3100, 13600], [3106, 13606], [3111, 13611], [3112, 13612], [3134, 13634], [3142, 13642], [3145, 13645], [3148, 13648], [8773, 19273], [3168, 13668], [3169, 13669], [8563, 19390], [9741, 14449], [3208, 13708], [3209, 13709], [4437, 14937], [3230, 15278], [6257, 16757], [3233, 13733], [3238, 13738], [3244, 13744], [3250, 13750], [3260, 13760], [9753, 20253], [3272, 13772], [3277, 13777], [5205, 15705], [3288, 13788], [5341, 15841], [3297, 13797], [4026, 14526], [3312, 13812], [7178, 17678], [7663, 18163], [10079, 20579], [8593, 19093], [3343, 13843], [3352, 13852], [3360, 13860], [3377, 13877], [3382, 13882], [3386, 13886], [3387, 13887], [6847, 17347], [3418, 13918], [3419, 13919], [7918, 18418], [7942, 14823], [10191, 20691], [3429, 13929], [9633, 20133], [3438, 13938], [3441, 13941], [3443, 13943], [3445, 13945], [3450, 13950], [4705, 15205], [3475, 13975], [6813, 17313], [3490, 13990], [3491, 13991], [5152, 15652], [5822, 16322], [9452, 19952], [3498, 13998], [9581, 20081], [3510, 16852], [4186, 14686], [3529, 11712], [10026, 20526], [10247, 20747], [3535, 20171], [3537, 14037], [3541, 14041], [3547, 14047], [3548, 14048], [2160, 12660], [9279, 19779], [1790, 12290], [3585, 11989], [3586, 14086], [1875, 12375], [5363, 15863], [3619, 14119], [3639, 14139], [3643, 14143], [8726, 19226], [3647, 12633], [1425, 11925], [3649, 14149], [10478, 20978], [3659, 14159], [9131, 19631], [3675, 14175], [6758, 17258], [3697, 14197], [2302, 12802], [4971, 15471], [9169, 19669], [3735, 14235], [3746, 14246], [8416, 18916], [9077, 19577], [3761, 14261], [9367, 19867], [6178, 18586], [3789, 14289], [5619, 16119], [6017, 16517], [3820, 14320], [1769, 12269], [8627, 19127], [3840, 14340], [3842, 36330], [4014, 14514], [3847, 14347], [3859, 14359], [3867, 14367], [3876, 14376], [3889, 14389], [3890, 14390], [3894, 14394], [4197, 14697], [5569, 16069], [3914, 14414], [3916, 14179], [3929, 11417], [9643, 20143], [8470, 18970], [3938, 14438], [3959, 14459], [3972, 17090], [3977, 14477], [3984, 14484], [3995, 14495], [6841, 17341], [5388, 15888], [10084, 20584], [4000, 13118], [4009, 14509], [4012, 14512], [9335, 19500], [9883, 20383], [3527, 14027], [4063, 14563], [6846, 17346], [4071, 14571], [4073, 14573], [4084, 14584], [4090, 14590], [4097, 14597], [6089, 16589], [7075, 17575], [10391, 20891], [4117, 14617], [4119, 14619], [4965, 15465], [4124, 14624], [7279, 17779], [7238, 17738], [4139, 14639], [2149, 12649], [7235, 17735], [4167, 14667], [8586, 19086], [4177, 14677], [4182, 14682], [8811, 19311], [9615, 20115], [9913, 20413], [8079, 18579], [4224, 12499], [4237, 14604], [4238, 14738], [5399, 15899], [4260, 14760], [8057, 18557], [4267, 14767], [9248, 19748], [9953, 20453], [4293, 14793], [1580, 12080], [4299, 14799], [6623, 17123], [4308, 14808], [4529, 15029], [4346, 14846], [5071, 15571], [4359, 14859], [4365, 14865], [5951, 16451], [4379, 14879], [4382, 14882], [4387, 14887], [4389, 14889], [4391, 14891], [4397, 14897], [4399, 14899], [8363, 37519], [4411, 14911], [7091, 17591], [4426, 14926], [4432, 14932], [5821, 16321], [4443, 14943], [4446, 11266], [4460, 11376], [4476, 14976], [4482, 14982], [4483, 14983], [4484, 14984], [4491, 14991], [5620, 16120], [6726, 15321], [4501, 15001], [10388, 20888], [9621, 20121], [8095, 18595], [4532, 15032], [4536, 15036], [6060, 16560], [7455, 17955], [4552, 15052], [8303, 18803], [657, 11157], [4579, 15079], [8455, 18955], [4591, 15091], [6035, 16535], [4600, 15100], [4606, 15106], [4608, 15108], [4625, 15125], [4626, 11007], [4628, 15128], [4629, 15129], [4637, 15137], [2256, 12756], [4662, 15162], [2568, 13068], [4690, 15190], [10199, 36968], [4687, 15187], [4694, 15194], [4702, 15202], [4718, 15218], [4720, 15220], [5837, 16337], [4739, 15239], [4747, 15247], [4758, 15258], [4759, 15259], [4769, 15269], [5198, 15698], [4786, 15286], [4798, 15298], [4809, 15309], [4837, 15337], [4841, 15341], [9617, 20117], [9656, 20156], [4852, 15352], [7463, 17963], [4865, 15365], [4870, 15370], [6200, 16700], [7517, 18017], [6670, 17170], [5276, 15776], [7960, 15633], [4929, 15429], [4946, 15446], [2966, 13466], [5370, 15870], [63, 10563], [5490, 15990], [3324, 13824], [4993, 15493], [4994, 15494], [5012, 15512], [5432, 15932], [6022, 16522], [8435, 18935], [5033, 15533], [4700, 15200], [5050, 15550], [5505, 16005], [5059, 15559], [5065, 15565], [5069, 15569], [5072, 15572], [5687, 10994], [5077, 15577], [2013, 12513], [5082, 15582], [5085, 15585], [7154, 17654], [5095, 15595], [5096, 15596], [5097, 15597], [8528, 19028], [5109, 15609], [9436, 19936], [1087, 11587], [5120, 15620], [10284, 20784], [5127, 15627], [5129, 15629], [5135, 15635], [5138, 15638], [2101, 12601], [5147, 15647], [5160, 15660], [5171, 15671], [5176, 15676], [5179, 15679], [5183, 15683], [717, 11217], [5192, 15692], [5204, 15704], [2475, 12975], [7970, 18470], [5214, 15714], [5230, 15730], [5255, 15755], [8897, 19397], [5283, 15783], [5934, 16434], [5291, 15791], [6497, 16997], [5309, 15809], [5318, 15818], [5328, 15828], [5329, 15829], [5333, 15833], [9382, 19882], [5366, 15866], [5368, 15868], [8905, 19405], [5378, 15878], [5404, 15904], [8996, 19496], [5409, 15909], [5739, 16239], [5421, 15921], [3855, 14355], [5425, 15925], [5434, 15934], [5442, 15942], [5476, 15976], [5537, 14685], [7439, 17939], [5564, 16064], [6131, 19979], [7538, 18038], [5580, 18531], [5584, 16084], [5586, 16086], [5872, 16372], [5618, 16118], [7306, 17806], [5626, 16126], [5634, 16134], [5637, 16137], [5641, 16141], [5649, 16149], [5650, 16150], [8904, 19404], [5659, 16159], [8332, 18832], [3326, 13826], [5670, 11848], [5677, 16177], [2430, 12930], [9306, 19806], [5731, 16231], [5733, 16233], [8091, 18591], [5750, 16250], [6678, 17178], [9850, 20350], [5760, 16260], [5763, 16263], [7610, 18110], [7619, 18119], [5780, 16280], [5790, 16290], [5792, 37749], [9556, 20056], [5861, 16361], [8732, 19232], [5819, 14368], [7444, 17944], [9576, 20076], [5849, 16349], [5855, 16355], [9292, 19792], [3947, 14447], [5905, 16405], [8592, 19092], [5909, 16409], [5911, 16411], [6605, 17105], [3989, 14489], [5925, 16425], [5947, 16447], [5953, 14502], [5957, 16457], [8552, 11781], [5972, 16472], [9691, 19985], [5989, 16489], [5994, 16494], [6005, 16505], [6008, 16508], [6009, 12537], [3874, 33977], [6058, 16558], [10431, 20931], [8658, 19158], [6071, 16571], [6085, 11125], [5531, 16031], [6098, 16598], [6099, 16599], [6102, 16602], [6109, 16609], [8745, 19245], [7414, 17914], [6119, 20810], [10124, 20624], [7119, 17619], [6150, 16650], [6151, 18247], [6164, 16664], [6181, 16681], [6183, 16683], [6193, 16693], [8780, 19280], [6325, 16825], [6208, 16708], [6209, 16709], [6212, 16712], [6221, 16721], [6252, 16752], [6264, 11832], [6270, 16770], [6277, 18814], [7039, 17539], [8287, 18787], [7679, 17187], [6315, 15219], [10072, 20572], [8258, 18758], [9437, 19937], [6331, 16831], [6335, 16835], [6340, 16840], [8869, 19369], [6354, 16854], [9602, 37730], [1031, 11531], [6395, 16895], [6418, 16918], [6425, 16925], [6431, 16931], [9749, 20249], [6444, 16944], [6456, 16956], [9134, 19634], [6464, 16964], [6470, 16970], [6475, 16975], [7382, 20932], [6795, 17295], [6513, 17013], [6527, 17027], [6549, 17049], [8913, 19413], [6551, 17051], [6556, 17056], [6566, 17066], [6666, 17166], [4551, 19249], [2543, 13043], [6590, 13025], [6603, 20148], [6608, 17108], [6620, 17120], [6621, 36361], [9699, 20199], [9762, 20262], [6633, 17133], [8148, 18146], [6644, 17144], [6652, 17152], [7940, 18440], [9037, 19537], [7744, 18244], [6675, 17175], [6698, 17198], [7749, 18249], [6734, 17234], [7985, 18485], [6762, 17262], [8855, 19355], [6778, 17278], [6800, 17300], [537, 11037], [6816, 17316], [6829, 17329], [6849, 17349], [10086, 20586], [6857, 17357], [6858, 17358], [6875, 17375], [8254, 18754], [6894, 17394], [8189, 18689], [6901, 17401], [5300, 15800], [6917, 17417], [8035, 18535], [6950, 17450], [6954, 17454], [7772, 18272], [8238, 18738], [6961, 17461], [6962, 17462], [6979, 17479], [8967, 19467], [7017, 17517], [7059, 35763], [8549, 19049], [7076, 17576], [7077, 15621], [8751, 19251], [7100, 17600], [2718, 13218], [8099, 18599], [7112, 17612], [7133, 17633], [7138, 17638], [7143, 17643], [7160, 17660], [7163, 17663], [7164, 17664], [5718, 16218], [7168, 17668], [7174, 17674], [7192, 17692], [8012, 18512], [7218, 17718], [7257, 17757], [7258, 17758], [7267, 17767], [7270, 17770], [7304, 17804], [7305, 17805], [7311, 17811], [7320, 14247], [8895, 19395], [7328, 12218], [7332, 17832], [7336, 17836], [7346, 17846], [8298, 18798], [7419, 16402], [7453, 17953], [7454, 17954], [7465, 17965], [7468, 17968], [6744, 17244], [7472, 17972], [7485, 17985], [6253, 16753], [9881, 20381], [7528, 18028], [7577, 18077], [7581, 18081], [7614, 18114], [7615, 18115], [5803, 16303], [8929, 19429], [7690, 18190], [7715, 18215], [9738, 20238], [8883, 19383], [7739, 18239], [7740, 15150], [7746, 18246], [8765, 19265], [9564, 20064], [7757, 18257], [7781, 18281], [7786, 18286], [9771, 20271], [7798, 18298], [7801, 18301], [7803, 18303], [7808, 18308], [7818, 18318], [2426, 12926], [7841, 11792], [7857, 18357], [7888, 18388], [7962, 18462], [7968, 18468], [7974, 18474], [7981, 18481], [8000, 18500], [5103, 15603], [8026, 18526], [8027, 18527], [8037, 18537], [8049, 18549], [8056, 18556], [8059, 18559], [8093, 18593], [8102, 18602], [8617, 19117], [8126, 18626], [9630, 20130], [8165, 18665], [8183, 18683], [8195, 18695], [8196, 18696], [8257, 18757], [9491, 19991], [8744, 19244], [8281, 18781], [8289, 18789], [8309, 18809], [8317, 18817], [8344, 18844], [8347, 18847], [8365, 18865], [8375, 16811], [8377, 18877], [8379, 18879], [8381, 18881], [8383, 18883], [8395, 18895], [8602, 19102], [8669, 19169], [10331, 20831], [8446, 18946], [8495, 18995], [8497, 18997], [8500, 19000], [8507, 19007], [8508, 19008], [8513, 19013], [9238, 19738], [6412, 16912], [8536, 19036], [8538, 19038], [9578, 11022], [10202, 20702], [8583, 12210], [8589, 13859], [8621, 19121], [8635, 19135], [7759, 18259], [8656, 19156], [8662, 19162], [8986, 20188], [8689, 19189], [8725, 35968], [8733, 19233], [8762, 19262], [8766, 19266], [8775, 19275], [8797, 19297], [8805, 19305], [8827, 19327], [8854, 19354], [7120, 17620], [9121, 19385], [8894, 19394], [8907, 19407], [8940, 19440], [2948, 13448], [8988, 19488], [8998, 19498], [9002, 19502], [10158, 20658], [9031, 19531], [9038, 19538], [9087, 19587], [9096, 19596], [9103, 19603], [9112, 19612], [9149, 19649], [9154, 19654], [9179, 19679], [9181, 19681], [9183, 19683], [9188, 20142], [9203, 19703], [9213, 19713], [9221, 19721], [9223, 19723], [9235, 12951], [9268, 19768], [9304, 35712], [3270, 13770], [9354, 19854], [9356, 19856], [9366, 19866], [9370, 19870], [9406, 19906], [9420, 11742], [9434, 19934], [9462, 19962], [9486, 36536], [9500, 20000], [9543, 20043], [9582, 20082], [9588, 20088], [9590, 20090], [9614, 20114], [9655, 20155], [9663, 11596], [9667, 20167], [9682, 20182], [9704, 20204], [9705, 20205], [9714, 20214], [9730, 20230], [7324, 17824], [7670, 18170], [9773, 20273], [9802, 20302], [9804, 20304], [9820, 20320], [9821, 18408], [9824, 20324], [8444, 11547], [9828, 20328], [9853, 11969], [9892, 20392], [9898, 20398], [9905, 20405], [9927, 20427], [6487, 16987], [9941, 20441], [9949, 20449], [9982, 20482], [9984, 20484], [10005, 20505], [10008, 20508], [10021, 17696], [10046, 20546], [10055, 12019], [10348, 20848], [10096, 20596], [10123, 20623], [10179, 20679], [10180, 20680], [10266, 20766], [10306, 20806], [10325, 20825], [10345, 20845], [10360, 20860], [10362, 20862], [10370, 20870], [10373, 20873], [10437, 20937], [10475, 20975], [10494, 20994], [10498, 20998], [10506, 6], [10513, 13], [15228, 4728], [10999, 499], [15054, 4554], [10514, 14], [10517, 17], [12768, 2268], [13855, 3355], [12596, 2096], [10524, 24], [10531, 31], [10743, 243], [15531, 5031], [10542, 42], [13234, 2734], [14926, 4426], [11432, 932], [10544, 44], [14238, 3738], [15256, 4756], [10546, 46], [16999, 6499], [17124, 6624], [14392, 3892], [16315, 5815], [10554, 54], [10556, 56], [18340, 7840], [10563, 63], [10565, 65], [11397, 897], [10572, 72], [10573, 73], [10575, 75], [10579, 79], [12011, 1511], [14890, 4390], [10580, 80], [10977, 477], [16842, 6342], [12100, 1600], [17307, 6807], [18390, 7890], [13096, 2596], [16586, 6086], [10591, 91], [18605, 8105], [14290, 3790], [16028, 5528], [10600, 100], [10601, 101], [10602, 102], [10603, 103], [10612, 112], [10617, 117], [10626, 126], [10990, 490], [12280, 1780], [14681, 4181], [18151, 7651], [10633, 133], [10641, 141], [14957, 4457], [10643, 143], [10944, 444], [12073, 1573], [17284, 6784], [10650, 150], [10663, 163], [10665, 165], [10667, 24240], [10672, 172], [10676, 176], [17618, 7118], [19289, 8789], [10678, 178], [17172, 6672], [10679, 179], [10686, 186], [18487, 7987], [10687, 9961], [12050, 1550], [17750, 7250], [10698, 198], [18045, 7545], [10703, 203], [18300, 7800], [12903, 2403], [10709, 209], [10712, 212], [11423, 923], [14588, 4088], [15871, 5371], [17104, 6604], [15235, 4735], [10719, 219], [10722, 222], [10723, 223], [10724, 224], [11199, 1323], [11356, 856], [10732, 232], [10735, 661], [11224, 724], [13731, 3231], [10739, 239], [17437, 6937], [17866, 7366], [19257, 8757], [11362, 5284], [17617, 7117], [10748, 248], [12945, 2445], [10753, 253], [17817, 7317], [10765, 265], [16849, 6349], [10770, 270], [10773, 273], [14091, 3591], [17816, 7316], [10782, 282], [10784, 284], [11185, 685], [18915, 8415], [19146, 8646], [10787, 287], [16072, 5572], [10789, 289], [10791, 291], [15007, 4507], [19029, 8529], [10793, 293], [16611, 6111], [10794, 294], [10805, 305], [10806, 306], [10808, 424], [17292, 6792], [10812, 312], [10815, 315], [17662, 7162], [10830, 330], [10831, 331], [10832, 332], [14782, 4282], [16284, 5784], [17118, 6618], [15432, 4932], [18954, 8454], [10868, 368], [10873, 373], [10875, 375], [13700, 23953], [10881, 381], [12253, 1753], [10882, 382], [10896, 396], [18662, 8162], [10908, 408], [16264, 5764], [18962, 8462], [10915, 415], [13522, 3022], [10916, 416], [11116, 616], [10918, 418], [15997, 5497], [16636, 6136], [17544, 7044], [10928, 428], [13242, 2742], [17675, 7175], [18645, 8145], [15814, 5314], [17705, 7205], [10963, 463], [11647, 1147], [10970, 470], [12560, 2060], [10976, 476], [17354, 6854], [10989, 489], [11041, 541], [12751, 2251], [14136, 3636], [14766, 4266], [19135, 8635], [10994, 5687], [11000, 500], [11001, 501], [11006, 506], [11007, 4626], [11008, 508], [11012, 512], [15433, 4933], [18998, 8498], [18980, 8480], [11021, 521], [11022, 9578], [11024, 524], [15358, 4858], [17491, 6991], [11034, 534], [12899, 2399], [14100, 3600], [11036, 536], [11044, 22466], [13476, 2976], [17248, 6748], [14836, 4336], [15841, 5341], [11064, 564], [14864, 4364], [11068, 25337], [11081, 581], [11083, 583], [15990, 5490], [11089, 589], [11090, 590], [11096, 596], [11100, 600], [11102, 602], [11106, 606], [11108, 608], [11111, 611], [11900, 1400], [11923, 1423], [15085, 4585], [18170, 7670], [11115, 615], [11118, 618], [11122, 622], [11448, 948], [14075, 3575], [14770, 4270], [15343, 4843], [15386, 4886], [15579, 5079], [17156, 6656], [11125, 6085], [11128, 628], [18561, 8061], [11132, 25133], [11138, 638], [16802, 6302], [13541, 3041], [18821, 8321], [11156, 656], [11159, 659], [11163, 663], [11598, 1098], [14233, 22984], [13701, 3201], [11170, 670], [15446, 4946], [16042, 5542], [16275, 5775], [17313, 6813], [18737, 8237], [11172, 672], [16070, 5570], [12594, 2094], [11183, 683], [11186, 686], [11189, 689], [14335, 3835], [11190, 690], [15463, 4963], [11191, 691], [11201, 701], [14652, 4152], [17342, 6842], [11206, 706], [11209, 709], [11216, 23484], [11230, 730], [11233, 733], [11234, 734], [18891, 8391], [11235, 735], [11237, 737], [13992, 3492], [11244, 744], [11933, 3401], [11247, 747], [11254, 938], [11562, 3999], [11259, 759], [11260, 760], [11266, 4446], [12143, 1643], [14135, 3635], [11273, 773], [12561, 2061], [11280, 780], [14922, 4422], [15701, 5201], [17607, 7107], [11284, 784], [11443, 943], [12109, 1609], [11292, 792], [11296, 435], [11300, 22386], [16492, 5992], [11305, 805], [12724, 2224], [18310, 7810], [11314, 814], [11315, 815], [11317, 817], [11325, 825], [12107, 1607], [11327, 827], [17213, 6713], [11638, 1138], [14084, 3584], [14387, 3887], [11343, 843], [11351, 851], [11353, 853], [11354, 854], [11358, 858], [11359, 2901], [15450, 4950], [11360, 860], [11366, 866], [11369, 3028], [11371, 871], [11376, 4460], [16473, 5973], [11378, 878], [11380, 880], [17180, 6680], [11388, 3816], [15943, 5443], [11404, 904], [11405, 905], [11410, 910], [15999, 5499], [18365, 7865], [11414, 914], [11417, 3929], [11419, 919], [15257, 4757], [11421, 921], [16672, 6172], [11439, 228], [18250, 7750], [13908, 3408], [18505, 8005], [19339, 8839], [11452, 952], [11456, 956], [11459, 959], [15455, 4955], [11462, 962], [11470, 970], [13534, 3034], [14205, 3705], [15133, 4633], [11473, 973], [11475, 975], [15092, 4592], [11480, 980], [11481, 981], [13907, 3407], [18090, 7590], [11484, 984], [16516, 6016], [11492, 992], [11503, 22776], [11561, 1061], [11510, 1010], [14308, 3808], [14237, 3737], [17239, 6739], [12257, 1757], [11517, 1017], [13351, 2851], [13801, 3301], [11521, 1021], [17627, 7127], [17877, 7377], [15776, 5276], [11527, 1027], [16648, 6148], [11530, 1030], [11537, 1037], [11540, 1040], [11543, 1043], [11545, 1045], [11547, 8444], [15502, 5002], [11564, 1064], [12195, 1695], [12885, 2385], [11568, 1068], [11575, 1075], [11577, 1077], [11579, 1079], [11584, 1084], [11596, 9663], [11601, 1101], [14152, 3652], [17343, 6843], [18490, 7990], [12203, 1703], [18230, 7730], [12718, 2218], [16826, 6326], [11614, 1114], [11618, 1118], [11626, 1126], [13524, 3024], [11629, 1129], [14143, 3643], [16981, 6481], [14577, 4077], [15472, 4972], [17448, 6948], [16808, 6308], [12088, 1588], [11648, 1148], [16853, 6353], [11654, 1154], [12810, 2310], [14164, 3664], [14688, 4188], [15869, 5369], [16671, 6171], [17058, 6558], [17328, 6828], [18568, 8068], [18906, 8406], [15616, 5116], [11669, 1169], [11674, 1174], [12204, 1704], [11675, 1175], [12569, 2069], [11677, 1177], [11679, 1179], [11683, 1183], [11687, 1720], [11689, 1189], [11695, 1195], [16521, 6021], [17161, 6661], [18584, 24191], [11703, 1203], [17892, 7392], [15611, 5111], [11712, 3529], [11713, 1213], [12049, 4770], [11714, 1214], [16673, 6173], [10904, 404], [11715, 1215], [17464, 6964], [17510, 7010], [11720, 1220], [11723, 1223], [11724, 1224], [16909, 6409], [15777, 5277], [11736, 1236], [11740, 6655], [11742, 9420], [11754, 1254], [11763, 1263], [11765, 1265], [12196, 1696], [14293, 3793], [11775, 1275], [16519, 6019], [11781, 8552], [12770, 2270], [16289, 5789], [16424, 5924], [13412, 2912], [11785, 1285], [11792, 7841], [11797, 1297], [11800, 1300], [11804, 3390], [11807, 1307], [14099, 3599], [17837, 7337], [11821, 1321], [11825, 1325], [11832, 6264], [13229, 2729], [14262, 3762], [11848, 5670], [18065, 7565], [13905, 24586], [11865, 1365], [10984, 484], [11870, 1370], [15454, 4954], [15617, 5117], [19001, 8501], [13267, 2767], [15079, 4579], [12663, 2163], [10653, 153], [14109, 3609], [11891, 1391], [15172, 4672], [11899, 1399], [11902, 1402], [11908, 1408], [11910, 1410], [11914, 1414], [11925, 1425], [11932, 1432], [11936, 323], [16352, 5852], [14850, 4350], [17123, 6623], [18884, 8384], [11950, 1450], [11961, 1461], [17714, 7214], [11966, 1466], [15678, 5178], [11969, 9853], [16270, 5770], [11975, 1475], [13258, 2758], [11981, 1481], [11984, 1484], [11988, 1488], [18085, 7585], [17174, 22502], [11993, 1493], [18229, 7729], [13644, 3144], [12002, 1502], [12004, 1504], [12010, 1510], [12013, 1513], [12018, 1518], [12019, 10055], [13285, 2785], [14689, 4189], [18617, 8117], [17610, 7110], [12035, 1535], [12037, 1537], [11018, 518], [13873, 3373], [14700, 4200], [19269, 8769], [12154, 1654], [12185, 1685], [15541, 5041], [14772, 4272], [12059, 1559], [12062, 1562], [15586, 5086], [12067, 1567], [12325, 1825], [16056, 5556], [12075, 1575], [14564, 4064], [12078, 1578], [18096, 7596], [12090, 1590], [17653, 7153], [12093, 1593], [12101, 1601], [12108, 1608], [19191, 8691], [12110, 1610], [13160, 2660], [18606, 8106], [12126, 2668], [12129, 1629], [12132, 1632], [12134, 1634], [16676, 6176], [15374, 4874], [12152, 1652], [12235, 1735], [12157, 1657], [12164, 1664], [12165, 1665], [15375, 4875], [12166, 1666], [12169, 1669], [13131, 2631], [12172, 1672], [12174, 1674], [12180, 1680], [17134, 6634], [12181, 1681], [15395, 4895], [16368, 5868], [12188, 1688], [15835, 5335], [12198, 1698], [12296, 1796], [12209, 1709], [16971, 6471], [12210, 8583], [12213, 1713], [12218, 7328], [12219, 1719], [19299, 8799], [17911, 7411], [12238, 24536], [12247, 1747], [12248, 1748], [12251, 1751], [16472, 5972], [19073, 8573], [14742, 4242], [12457, 1957], [13111, 2611], [12268, 1768], [12270, 2553], [12271, 1771], [12273, 1773], [12288, 2615], [17277, 6777], [12299, 1799], [10983, 483], [19079, 8579], [15470, 4970], [12313, 1813], [19253, 8753], [12317, 1817], [17959, 7459], [12323, 1823], [12328, 1828], [16632, 6132], [17511, 7011], [12329, 1829], [12335, 1835], [11468, 968], [12962, 2462], [18689, 8189], [11512, 1012], [12344, 1844], [13322, 2822], [18294, 7794], [14792, 4292], [12354, 1854], [12355, 1855], [12556, 2056], [17007, 6507], [13783, 3283], [12376, 1923], [12379, 1879], [18084, 7584], [12394, 1894], [18721, 8221], [18800, 8300], [12407, 1907], [12409, 1909], [12410, 1910], [12548, 2048], [12719, 2219], [14575, 4075], [12420, 1920], [16348, 5848], [17023, 6523], [16163, 5663], [12445, 1945], [10642, 142], [12447, 1947], [14059, 3559], [17556, 7056], [12451, 1951], [15139, 4639], [12459, 1959], [16685, 6185], [12461, 1961], [16304, 5804], [11433, 933], [12467, 1967], [18210, 7710], [17475, 6975], [14747, 4247], [12476, 1976], [12482, 1982], [12486, 1986], [12488, 1988], [12492, 1992], [13336, 2836], [12496, 1996], [12499, 4224], [13458, 2958], [16469, 5969], [12507, 2007], [15811, 5311], [12514, 2014], [12517, 2017], [12529, 5288], [12533, 2033], [12536, 2036], [15960, 5460], [12537, 6009], [16539, 6039], [12933, 2433], [12553, 2053], [14042, 3542], [12565, 2065], [15540, 5040], [18592, 8092], [12577, 2077], [12582, 2082], [15122, 4622], [12587, 2087], [12591, 2091], [12592, 2092], [12593, 2093], [12595, 2095], [14221, 3721], [12937, 2437], [12600, 2100], [12601, 2101], [12608, 2108], [12611, 4961], [14229, 3729], [12620, 2120], [15464, 4964], [12633, 3647], [14036, 3536], [12638, 22121], [10710, 210], [19381, 8881], [12645, 2145], [12647, 2147], [12648, 2148], [12649, 2149], [12650, 2150], [15812, 5312], [12653, 6870], [17015, 6515], [12662, 2162], [14265, 3765], [15688, 5188], [17558, 7058], [12664, 2164], [17991, 7491], [15820, 5320], [12672, 2172], [16277, 4568], [12685, 2185], [15779, 5279], [12686, 2186], [17032, 6532], [17336, 6836], [12691, 2191], [18360, 7860], [12693, 2193], [12695, 2195], [13277, 2777], [13423, 2923], [16039, 647], [16303, 5803], [12708, 25170], [13967, 3467], [12721, 2221], [12733, 2233], [14263, 3763], [15447, 4947], [18372, 7872], [12736, 2236], [12741, 2241], [12743, 2243], [12746, 2246], [16078, 5578], [15474, 4974], [16168, 5668], [18996, 8496], [16387, 5887], [17263, 6763], [12754, 2254], [12756, 2256], [12758, 2258], [12759, 2259], [18588, 8088], [19347, 8847], [19148, 8648], [12774, 2274], [17971, 7471], [13449, 2949], [12778, 23046], [18903, 8403], [12779, 2279], [12782, 2282], [12784, 2284], [14779, 4279], [12787, 2287], [17960, 7460], [13110, 2610], [12797, 2297], [12808, 2308], [13408, 2908], [17367, 6867], [14049, 3549], [12830, 2330], [18921, 8421], [14802, 4302], [12835, 2335], [17635, 7135], [13954, 3454], [15874, 5374], [17385, 6885], [12873, 2373], [14939, 4439], [12877, 2377], [12878, 2378], [14821, 4321], [12888, 2388], [12890, 2390], [12893, 2393], [12895, 2395], [13661, 3161], [12905, 2405], [15202, 4702], [14412, 3912], [17042, 6542], [12910, 2410], [13452, 2952], [15930, 5430], [19154, 8654], [12917, 2417], [12925, 2425], [12926, 2426], [12935, 2376], [19209, 8709], [13179, 2679], [12950, 2450], [12951, 9235], [12960, 2460], [12963, 2463], [18472, 7972], [12976, 2476], [12978, 2478], [12987, 2487], [14719, 4219], [12994, 2494], [15401, 4901], [13009, 371], [13015, 2515], [18181, 7681], [13025, 6590], [13026, 2526], [13028, 2528], [14817, 4317], [16483, 5983], [18904, 8404], [13622, 3122], [13037, 2537], [13288, 2788], [14920, 4420], [13040, 2540], [16228, 5728], [13056, 2556], [17735, 7235], [13057, 2557], [17590, 7090], [18989, 8489], [13064, 2564], [13070, 2570], [13075, 2575], [13080, 2580], [13081, 2581], [13082, 2582], [15151, 4651], [15324, 4824], [17154, 6654], [13167, 2667], [17608, 7108], [13095, 2595], [13098, 2598], [13838, 21521], [17526, 7026], [13102, 2602], [17383, 6883], [13104, 2604], [13297, 2797], [13118, 4000], [15643, 5143], [13121, 2621], [13122, 1770], [13136, 2636], [13138, 2638], [18336, 7836], [13174, 2674], [13180, 2680], [13182, 2682], [14130, 3630], [14592, 4092], [13185, 2685], [13188, 2688], [13190, 2690], [17355, 6855], [13203, 2703], [14503, 4003], [13212, 2712], [10736, 236], [14318, 3818], [13235, 2735], [13246, 2746], [14052, 3552], [13256, 2756], [13539, 3039], [13272, 2772], [13274, 2774], [13633, 3133], [16554, 6054], [17757, 7257], [13286, 2786], [13289, 2789], [13292, 2792], [13294, 24329], [13299, 2799], [16571, 6071], [13325, 2825], [18391, 7891], [13358, 2858], [14282, 3782], [13331, 2831], [14933, 4433], [14980, 4480], [13338, 2838], [13342, 2842], [13352, 2852], [13353, 2853], [16607, 6107], [16822, 6322], [13370, 2870], [13378, 2878], [13392, 2892], [13393, 2893], [12946, 2446], [13400, 2900], [14444, 3944], [13411, 2911], [13414, 2914], [14299, 3799], [15249, 4749], [16843, 6343], [17819, 7319], [19152, 8652], [13420, 2920], [13431, 2931], [15846, 5346], [10688, 188], [13441, 2941], [13450, 2950], [14095, 3595], [13276, 2776], [13464, 2964], [15713, 5213], [13479, 23105], [10631, 131], [13482, 2982], [19226, 8726], [13486, 2986], [17330, 6830], [13488, 2988], [18479, 7979], [13491, 3400], [13498, 2998], [13502, 3002], [14465, 3965], [17117, 6617], [13506, 3006], [16514, 6014], [13512, 3012], [13523, 3023], [13527, 3027], [15786, 5286], [15965, 5465], [13532, 3032], [13535, 3035], [13547, 3047], [13554, 22439], [13559, 3059], [13560, 3060], [13562, 3062], [13570, 3070], [13572, 3072], [16278, 5778], [19162, 8662], [14517, 4017], [13600, 3100], [13601, 3101], [13606, 3106], [17468, 24758], [13611, 3111], [13612, 3112], [13623, 3123], [13624, 3124], [13634, 3134], [13642, 3142], [15808, 1501], [13645, 3145], [13648, 3148], [14651, 4151], [13649, 3149], [13654, 3154], [19111, 8611], [19273, 8773], [13664, 9966], [13668, 3168], [13669, 3169], [14449, 9741], [13695, 3195], [16498, 5998], [13708, 3208], [13709, 3209], [13713, 3213], [13719, 3219], [14937, 4437], [16757, 6257], [13733, 3233], [13738, 3238], [13744, 3244], [13750, 3250], [13760, 3260], [13772, 3272], [13777, 3277], [13778, 3278], [15705, 5205], [13785, 3285], [13788, 3288], [17614, 7114], [13794, 3568], [13797, 3297], [14526, 4026], [13805, 3305], [13812, 3312], [15978, 5478], [17678, 7178], [18163, 7663], [13818, 3318], [15110, 729], [18457, 7957], [19093, 8593], [16480, 5980], [13843, 3343], [13847, 3347], [13852, 3352], [13853, 3353], [13859, 8589], [13860, 3360], [13877, 3377], [13882, 3382], [13884, 3384], [13886, 3386], [13887, 3387], [13888, 3388], [17347, 6847], [13909, 3409], [13910, 3410], [18187, 7687], [13918, 3418], [13919, 3419], [18418, 7918], [13929, 3429], [13938, 3438], [13941, 3441], [13943, 3443], [13945, 3445], [13950, 3450], [15205, 4705], [13975, 3475], [13978, 3478], [12827, 2327], [13989, 3489], [13990, 3490], [13991, 3491], [15652, 5152], [16322, 5822], [13996, 3496], [13998, 3498], [14015, 3515], [14686, 4186], [14037, 3537], [14041, 3541], [14045, 3545], [14048, 3548], [17917, 7417], [11435, 935], [12660, 2160], [12290, 1790], [14073, 3573], [15230, 4730], [14086, 3586], [15063, 4563], [12375, 1875], [15863, 5363], [14119, 3619], [14139, 3639], [14148, 3648], [14149, 3649], [19237, 8737], [14153, 3653], [14159, 3659], [14162, 3662], [12302, 1802], [17401, 6901], [16626, 6126], [14175, 3675], [14179, 3916], [17258, 6758], [14197, 3697], [12802, 2302], [15471, 4971], [14220, 1144], [14235, 3735], [14246, 3746], [14247, 7320], [14248, 3748], [18916, 8416], [10930, 430], [14250, 3750], [14255, 3755], [14257, 157], [14261, 3761], [14266, 1427], [14300, 3800], [16119, 5619], [16517, 6017], [17812, 7312], [14320, 3820], [10625, 125], [14326, 3826], [15127, 4627], [12269, 1769], [19127, 8627], [14340, 3840], [14514, 4014], [18990, 8490], [14347, 3847], [14359, 3859], [14367, 3867], [14368, 5819], [14376, 3876], [14389, 3889], [14390, 3890], [14394, 3894], [14399, 3899], [14697, 4197], [16069, 5569], [17096, 6596], [17325, 6825], [18896, 8396], [19228, 8728], [14414, 3914], [12866, 2366], [11347, 847], [14418, 3918], [15891, 5391], [14428, 3928], [14437, 3937], [18970, 8470], [14438, 3938], [14459, 3959], [14475, 3975], [14477, 3977], [14484, 3984], [14487, 3987], [17341, 6841], [15888, 5388], [14502, 5953], [14508, 2724], [14509, 4009], [14512, 4012], [15605, 5105], [18964, 8464], [18128, 7628], [14520, 4020], [14027, 3527], [14558, 4058], [18354, 7854], [14563, 4063], [17346, 6846], [14571, 4071], [14573, 4073], [14579, 4079], [14584, 4084], [14590, 4090], [14597, 4097], [16589, 6089], [14604, 4237], [17575, 7075], [14617, 4117], [14618, 4118], [14619, 4119], [13251, 2751], [14620, 4120], [15465, 4965], [14624, 4124], [17779, 7279], [17738, 7238], [17049, 6549], [14639, 4139], [14667, 4167], [19086, 8586], [14677, 4177], [14682, 4182], [14685, 5537], [14707, 4207], [19311, 8811], [14710, 4210], [18579, 8079], [14738, 4238], [14739, 4239], [15899, 5399], [17873, 7373], [14744, 4244], [15910, 5410], [14754, 4254], [14755, 4255], [14760, 4260], [14762, 4262], [14765, 4265], [16460, 5960], [18557, 8057], [14767, 4267], [14778, 4278], [19012, 8512], [15017, 4517], [14793, 4293], [14794, 4294], [12080, 1580], [14799, 4299], [14808, 4308], [14809, 4309], [14823, 7942], [19180, 8680], [14824, 4324], [14831, 4331], [15496, 4996], [18302, 7802], [18413, 7913], [14838, 4338], [15029, 4529], [15334, 4339], [14841, 4341], [14625, 4125], [14846, 4346], [15571, 5071], [14853, 4353], [14859, 4359], [11294, 794], [14865, 4365], [16451, 5951], [14873, 4373], [14879, 4379], [14882, 4382], [14887, 4387], [14889, 4389], [14891, 4391], [15241, 4741], [14894, 4394], [14897, 4397], [14899, 4399], [11373, 873], [14902, 4402], [14904, 104], [14911, 4411], [17496, 2538], [17591, 7091], [14289, 3789], [16664, 6164], [19249, 4551], [14927, 4427], [14932, 4432], [16321, 5821], [14943, 4443], [14976, 4476], [14982, 4482], [14983, 4483], [14984, 4484], [14991, 4491], [16120, 5620], [14992, 4492], [15001, 4501], [15018, 4518], [18595, 8095], [15032, 4532], [15036, 4536], [15041, 4541], [16560, 6060], [17955, 7455], [15052, 4552], [18803, 8303], [15075, 4575], [15090, 4590], [18955, 8455], [15091, 4591], [16535, 6035], [15100, 4600], [15102, 4602], [15106, 4606], [15107, 4607], [15108, 4608], [15111, 4611], [15125, 4625], [15128, 4628], [15129, 4629], [15136, 4636], [15137, 4637], [15150, 7740], [15157, 4657], [15162, 4662], [13068, 2568], [15168, 4668], [15190, 4690], [15180, 2113], [15187, 4687], [15194, 4694], [15200, 4700], [15218, 4718], [15220, 4720], [16337, 5837], [15239, 4739], [17836, 7336], [15246, 1731], [15247, 4747], [15248, 21638], [15258, 4758], [15259, 4759], [15267, 4767], [15269, 4769], [15272, 4772], [16960, 6460], [15500, 5000], [15278, 3230], [15698, 5198], [15279, 4360], [15281, 2743], [15286, 4786], [15298, 4798], [15309, 4809], [15321, 6726], [15328, 4828], [15332, 4832], [15337, 4837], [16166, 5666], [15340, 4840], [15341, 4841], [17741, 7241], [15344, 4844], [17665, 7165], [15352, 4852], [17963, 7463], [15362, 4862], [15365, 4865], [15370, 4870], [16700, 6200], [15384, 4884], [18017, 7517], [17170, 6670], [15429, 4929], [18109, 7609], [17257, 6757], [13824, 3324], [15942, 5442], [16002, 5502], [18349, 7849], [19028, 8528], [15493, 4993], [15494, 4994], [15505, 5005], [15512, 5012], [15932, 5432], [18935, 8435], [15533, 5033], [15550, 5050], [16005, 5505], [15559, 5059], [15565, 5065], [15569, 5069], [15572, 5072], [15573, 5073], [15577, 5077], [15582, 5082], [15585, 5085], [17654, 7154], [15595, 5095], [15596, 5096], [15597, 5097], [17648, 10466], [15609, 5109], [15610, 5110], [17567, 7067], [11587, 1087], [15620, 5120], [15621, 7077], [15622, 5122], [14912, 4412], [15627, 5127], [15628, 5128], [16212, 5712], [15629, 5129], [15633, 7960], [15635, 5135], [15638, 5138], [15639, 5139], [17126, 6626], [15647, 5147], [17481, 6981], [15648, 5148], [15654, 5154], [15660, 5160], [15665, 5165], [15666, 5166], [16752, 6252], [15671, 5171], [15676, 5176], [15679, 5179], [15680, 5180], [15683, 5183], [11217, 717], [15692, 5192], [15697, 5197], [15703, 5203], [15704, 5204], [12975, 2475], [18470, 7970], [15714, 5214], [15715, 5215], [15730, 5230], [15744, 5244], [15755, 5255], [18260, 21969], [15783, 5283], [16340, 5840], [16434, 5934], [15791, 5291], [15792, 5292], [15796, 5296], [18928, 8428], [17875, 7375], [15798, 5298], [16997, 6497], [15809, 5309], [15818, 5318], [15828, 5328], [15829, 5329], [15833, 5333], [15870, 5370], [15858, 21647], [15866, 5366], [15868, 5368], [15881, 5381], [15873, 5373], [15878, 5378], [15886, 5386], [17493, 6993], [15904, 5404], [15906, 5406], [15909, 5409], [15920, 5420], [16239, 5739], [15921, 5421], [14355, 3855], [15925, 5425], [15934, 5434], [15966, 5466], [16502, 6002], [15969, 5469], [15975, 5475], [15976, 5476], [15985, 5485], [12513, 2013], [16003, 1618], [16004, 5504], [16007, 5507], [16023, 5523], [16155, 5655], [17939, 7439], [16062, 21069], [16064, 5564], [15968, 5468], [11020, 7595], [16074, 5574], [16084, 5584], [16086, 5586], [16087, 5587], [16372, 5872], [17823, 25217], [16100, 5600], [16118, 5618], [16123, 5623], [17806, 7306], [16126, 5626], [11536, 1036], [16129, 8110], [16134, 5634], [16137, 5637], [16141, 5641], [16149, 5649], [16150, 5650], [16159, 5659], [14495, 3995], [18832, 8332], [13826, 3326], [16177, 5677], [12930, 2430], [14187, 3687], [17969, 7469], [16203, 5703], [16210, 22832], [16214, 5714], [13326, 2826], [16220, 5720], [16231, 5731], [16233, 5733], [16238, 5738], [18591, 8091], [16249, 5749], [16250, 5750], [16255, 5755], [17178, 6678], [16256, 5756], [16260, 5760], [16263, 5763], [16265, 5765], [18110, 7610], [18844, 8344], [18119, 7619], [16280, 5780], [16290, 5790], [16293, 5793], [16361, 5861], [19232, 8732], [17944, 7444], [16355, 5855], [16363, 5863], [14447, 3947], [16402, 7419], [16405, 5905], [19092, 8592], [16409, 5909], [16411, 5911], [17105, 6605], [14489, 3989], [16425, 5925], [16447, 5947], [16567, 6067], [16449, 5949], [16457, 5957], [16487, 7737], [16489, 5989], [16494, 5994], [16505, 6005], [16508, 6008], [14251, 3751], [16522, 6022], [17756, 24601], [16550, 6050], [16551, 6051], [16558, 6058], [19158, 8658], [16584, 6084], [16031, 5531], [16597, 6097], [16598, 6098], [16599, 6099], [16602, 6102], [16608, 6108], [16609, 6109], [19245, 8745], [17914, 7414], [16650, 6150], [18038, 7538], [16661, 6161], [16674, 6174], [16681, 6181], [16683, 6183], [16693, 6193], [19280, 8780], [16825, 6325], [16708, 6208], [16709, 6209], [16712, 6212], [16721, 6221], [16770, 6270], [16784, 6284], [16791, 6291], [17539, 7039], [18986, 8486], [18787, 8287], [12117, 1617], [16798, 6298], [17187, 7679], [16810, 6310], [16811, 8375], [16829, 6329], [16831, 6331], [16835, 6335], [16840, 6340], [16845, 6345], [19369, 8869], [16852, 3510], [16856, 6356], [16859, 6359], [16867, 6367], [11531, 1031], [16870, 6370], [16895, 6395], [16918, 6418], [16925, 6425], [16927, 6427], [16931, 6431], [16944, 6444], [16637, 6137], [16956, 6456], [15219, 6315], [16964, 6464], [16968, 6468], [16970, 6470], [16975, 6475], [16995, 6495], [18232, 7732], [17295, 6795], [17011, 6511], [17013, 6513], [17027, 6527], [17051, 6551], [17056, 6556], [17064, 5408], [17066, 6566], [17166, 6666], [13043, 2543], [17086, 6586], [17088, 24761], [17090, 3972], [17106, 6606], [17108, 6608], [17109, 6609], [17120, 6620], [17133, 6633], [11157, 657], [17144, 6644], [17152, 6652], [14198, 3698], [18440, 7940], [18244, 7744], [17175, 6675], [17196, 6696], [17198, 6698], [17229, 6729], [17234, 6734], [18485, 7985], [17262, 6762], [17266, 6766], [19355, 8855], [17278, 6778], [17285, 25462], [17298, 6798], [17300, 6800], [18547, 8047], [11037, 537], [17315, 6815], [17316, 6816], [17329, 6829], [14358, 3858], [17340, 6840], [17349, 6849], [17357, 6857], [17358, 6858], [17369, 1570], [17375, 6875], [18754, 8254], [17386, 6886], [17394, 6894], [15800, 5300], [17417, 6917], [17422, 6922], [17428, 1668], [17454, 6954], [18272, 7772], [18738, 8238], [17461, 6961], [17467, 6967], [17474, 21945], [17479, 6979], [18611, 8111], [17485, 6985], [19258, 8758], [17517, 7017], [14203, 3703], [17525, 7025], [17559, 23652], [19049, 8549], [17576, 7076], [19251, 8751], [17597, 21373], [17598, 7098], [17600, 7100], [13218, 2718], [18599, 8099], [17609, 5541], [17612, 7112], [17619, 7119], [17620, 7120], [17633, 7133], [17638, 7138], [17643, 7143], [17656, 7156], [17659, 7159], [17663, 7163], [17664, 7164], [16218, 5718], [17668, 7168], [17674, 7174], [17692, 7192], [17696, 10021], [18512, 8012], [17702, 1389], [17718, 7218], [17719, 7219], [17758, 7258], [17764, 7264], [17767, 7267], [17770, 7270], [17788, 7288], [17789, 22077], [17804, 7304], [17805, 7305], [17811, 7311], [17832, 7332], [17842, 7342], [17846, 7346], [17847, 7347], [18798, 8298], [17953, 7453], [17954, 7454], [17965, 7465], [17968, 7468], [17244, 6744], [17972, 7472], [17985, 7485], [17987, 7487], [16753, 6253], [18002, 5028], [18030, 7530], [18077, 7577], [18078, 7578], [18081, 7581], [18089, 3007], [18114, 7614], [18115, 7615], [18536, 8036], [18144, 7644], [18146, 8148], [18150, 7650], [15686, 5186], [18166, 7666], [18169, 7669], [18186, 7686], [18189, 7689], [18190, 7690], [18215, 7715], [18235, 23418], [19383, 8883], [18239, 7739], [18246, 7746], [18247, 6151], [18249, 7749], [19265, 8765], [18257, 7757], [18258, 7758], [18281, 7781], [18286, 7786], [18289, 7789], [18298, 7798], [18301, 7801], [18303, 7803], [18308, 7808], [18318, 7818], [18320, 7820], [18886, 8386], [18356, 7856], [18357, 7857], [18362, 7862], [18370, 7870], [18388, 7888], [18394, 7894], [18408, 9821], [15675, 5175], [18450, 7950], [18462, 7962], [18468, 7968], [18474, 7974], [18481, 7981], [18500, 8000], [15603, 5103], [18526, 8026], [18527, 8027], [18531, 5580], [18535, 8035], [18537, 8037], [18549, 8049], [18556, 8056], [18559, 8059], [18586, 6178], [18593, 8093], [18601, 8101], [18852, 8352], [18602, 8102], [19117, 8617], [18626, 8126], [18631, 8131], [18644, 8144], [18654, 8154], [17130, 6630], [18660, 8160], [18665, 8165], [18683, 8183], [18684, 8184], [18695, 8195], [18696, 8196], [18757, 8257], [18758, 8258], [18768, 8268], [19244, 8744], [18781, 8281], [18789, 8289], [18809, 8309], [18814, 6277], [18817, 8317], [18847, 8347], [18851, 8351], [18865, 8365], [18877, 8377], [18879, 8379], [18881, 8381], [18883, 8383], [18895, 8395], [19102, 8602], [19169, 8669], [18918, 8418], [12979, 2479], [18934, 8434], [18946, 8446], [18973, 8473], [13446, 2946], [18995, 8495], [18997, 8497], [19000, 8500], [19002, 8502], [19006, 24553], [19007, 8507], [19008, 8508], [19013, 8513], [16912, 6412], [19036, 8536], [19038, 8538], [19045, 8545], [19055, 23018], [19070, 8570], [19076, 8576], [19077, 8577], [19108, 8608], [19121, 8621], [19134, 8634], [18259, 7759], [19156, 8656], [19307, 8807], [19188, 8688], [19189, 8689], [19197, 8697], [19217, 8717], [19233, 8733], [19262, 8762], [19266, 8766], [19275, 8775], [19294, 8794], [19297, 8797], [19305, 8805], [19327, 8827], [19354, 8854], [19357, 8857], [19385, 9121], [13448, 2948], [16246, 21772], [13770, 3270], [16797, 6297], [17690, 7190], [14020, 3520], [11989, 3585], [17824, 7324], [17660, 7160], [14047, 3547], [16987, 6487], [11077, 577], [13466, 2966], [17831, 7331], [7272, 17772], [7533, 18033], [8280, 18780], [10143, 20643], [10214, 20714], [5952, 16452], [3717, 14217], [1499, 11999], [6770, 17270], [6530, 17030], [5356, 15856], [9677, 20177], [3975, 14475], [6484, 16984], [3034, 13534], [4427, 14927], [1220, 11720], [5268, 15768], [4309, 14809], [7559, 18059], [6091, 16591], [18033, 7533], [15768, 5268], [17030, 6530], [11876, 1376], [17270, 6770], [11999, 1499], [17551, 7051], [18780, 8280], [18059, 7559], [14217, 3717], [11518, 1018], [17039, 6539], [12215, 1715], [16349, 5849], [15856, 5356], [10790, 290], [16591, 6091], [17886, 7386], [17450, 6950], [12063, 1563], [13221, 2721], [17772, 7272], [16854, 6354], [12327, 1827], [16984, 6484], [12031, 1531], [17632, 27298], [11968, 1468], [10998, 498], [17462, 6962], [17365, 6865], [18153, 7653], [18028, 7528], [16452, 5952]]\n",
            "3500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmap[15228]\n",
        "tmap[4728]"
      ],
      "metadata": {
        "id": "sNANe-bUsfC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tmap[15228])\n",
        "print(tmap[4728])"
      ],
      "metadata": {
        "id": "R4mxoBm_q33E",
        "outputId": "c00883f6-dcac-4940-da87-f6d6ce52242e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{4728: 7, 8262: 1, 10462: 1, 21147: 1, 22055: 1, 24593: 1, 25102: 1, 3301: 1, 6173: 1, 4507: 1, 5578: 1, 21710: 1, 21234: 1, 23773: 1, 1757: 2, 2061: 1, 4672: 1, 3789: 1, 22843: 2, 8757: 1}\n",
            "{15228: 7, 20962: 1, 34711: 1, 35048: 1, 35083: 1, 35324: 1, 13801: 1, 16673: 1, 34601: 1, 35119: 1, 12257: 2, 12561: 1, 15172: 1, 14289: 1, 34764: 3, 19257: 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(num):\n",
        "    train_pair_dic1 = dict()\n",
        "    for a,b in new_train_pair : \n",
        "        train_pair_dic1[a] = b\n",
        "        train_pair_dic1[b] = a\n",
        "    selected_rmap = get_match_relations(new_train_pair,threshold)\n",
        "\n",
        "    si = dict()\n",
        "    so = dict()\n",
        "\n",
        "    for h,t,r in radj:\n",
        "      if (t,r) not in si:\n",
        "        si[(t,r)] = [h]\n",
        "      else:\n",
        "        si[(t,r)] += [h]\n",
        "\n",
        "      if (h,r) not in so:\n",
        "        so[(h,r)] = [t]\n",
        "      else:\n",
        "        so[(h,r)] += [t]\n",
        "\n",
        "    lmap = dict()\n",
        "    rmap = dict()\n",
        "    tmap = dict()\n",
        "    for h,t,r in radj:\n",
        "        if h in train_pair_dic1 and r in selected_rmap :\n",
        "            if (train_pair_dic1[h],selected_rmap[r]) in so:\n",
        "                for t1 in so[(train_pair_dic1[h],selected_rmap[r])]:\n",
        "                    if t not in tmap:\n",
        "                        tmap[t] = dict()\n",
        "                    if t1 not in tmap[t]:\n",
        "                        tmap[t][t1] = 1\n",
        "                    else:\n",
        "                        tmap[t][t1] += 1\n",
        "\n",
        "        if t in train_pair_dic1 and r in selected_rmap  :\n",
        "            if (train_pair_dic1[t],selected_rmap[r]) in si:\n",
        "                for h1 in si[(train_pair_dic1[t],selected_rmap[r])]:\n",
        "                    if h not in tmap:\n",
        "                        tmap[h] = dict()\n",
        "                    if h1 not in tmap[h]:\n",
        "                        tmap[h][h1] = 1\n",
        "                    else:\n",
        "                        tmap[h][h1] += 1\n",
        "    l = []\n",
        "    for a in tmap:\n",
        "        if a < num_entity_1 and a not in train_pair_dic :\n",
        "            z1 = getMax(tmap[a]);\n",
        "            if  z1 in tmap and a == getMax(tmap[z1]):\n",
        "              train_pair_dic1[a] = z1\n",
        "              l.append([a,z1 ])\n",
        "          \n",
        "\n",
        "    # for a,b in pairs:\n",
        "    #     if a not in train_pair_dic :\n",
        "    #       if a in tmap:\n",
        "    #           train_pair_dic1[a] = getMax(tmap[a])\n",
        "    #           l.append([a,train_pair_dic1[a] ])\n",
        "    #       elif a in lmap:\n",
        "    #           train_pair_dic1[a] = getMax(lmap[a])\n",
        "    #           l.append([a,train_pair_dic1[a] ])\n",
        "    #       elif a in rmap:\n",
        "    #           train_pair_dic1[a] = getMax(rmap[a])\n",
        "    #           l.append([a,train_pair_dic1[a]])\n",
        "\n",
        "    s = 0\n",
        "    if (len(l) == 0):\n",
        "        return train_pair_dic1;\n",
        "    new_train_pair = np.concatenate([new_train_pair,np.array(l)],axis=0)\n",
        "    # for t in train_pair_dic1:\n",
        "    #     if t not in train_pair_dic and pair_dic[t] == train_pair_dic1[t]:\n",
        "    #         s += 1\n",
        "    # print(s/(len(pair_dic)-len(train_pair_dic)),len(train_pair_dic1)-len(train_pair_dic))"
      ],
      "metadata": {
        "id": "GMONjxLNp7xD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EA.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}